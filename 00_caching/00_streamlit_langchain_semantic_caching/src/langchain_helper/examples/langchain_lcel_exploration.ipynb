{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demystifying Langchain Expression Language\n",
    "\n",
    "https://blog.gopenai.com/demistfying-langchain-expression-language-5199f97c5fb9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conventional Langchain setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Give me small report about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=model,\n",
    "    output_parser=output_parser\n",
    ")\n",
    "\n",
    "result = chain.run(topic=\"Large Multimodal Model\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Expression Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the chain in an alternative manner using pipe operators (|) LCEL way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Give me small report about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI(\n",
    "    temperature=0,\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using LCEL \n",
    "\n",
    "lcel_chain = prompt | model | output_parser\n",
    "\n",
    "# and run\n",
    "out = lcel_chain.invoke({\"topic\": \"Large Multimodal Model\"})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Runnable and pipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax follows standard Linux piping, introduced in Python. The | operator takes output from the left and feeds it into the function on the right.\n",
    "\n",
    "When the Python interpreter encounters the | operator connecting two objects, such as obj1 | obj2, it invokes the or method of object obj2 by passing object obj1 as an argument. This implies that the following patterns are interchangeable. Keeping this in mind, we can create a Runnable class that takes a function, transforming it into a chainable function using the pipe operator |. This forms the essence of LCEL.\n",
    "\n",
    "Below is an example that explains this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            return other(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)\n",
    "\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "def add_two(x):\n",
    "    return x + 2\n",
    "\n",
    "chain_plain = Runnable(add_one)\n",
    "print(chain_plain(3))\n",
    "\n",
    "# run them using the object approach\n",
    "chain_or = Runnable(add_one).__or__(Runnable(add_two))\n",
    "print(chain_or(3))\n",
    "\n",
    "# run  using a|b\n",
    "chain_pipe = Runnable(add_one) | (Runnable(add_two))\n",
    "print(chain_pipe(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring common prompt tasks and scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To regulate text generation in your chain, you have the option to incorporate stop sequences. In this configuration, the process of generating text concludes when a newline character is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bind refers to the concept of passing arguments during execution of the chain`\n",
    "chain = prompt | model.bind(stop=[\"\\n\"])\n",
    "result = chain.invoke({\"topic\": \"Large Multimodal Model\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LCEL facilitates the attachment of function call information to your chain, enhancing the functionality and providing valuable context during text generatio.This example attaches function call information to generate a summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"summary\",\n",
    "        \"description\": \"A summary\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"setup\": {\"type\": \"string\", \"description\": \"LMM summary\"},\n",
    "                \"punchline\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Summary\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"setup\", \"punchline\"],\n",
    "        },\n",
    "    }\n",
    "]\n",
    "chain = prompt | model.bind(function_call={\"name\": \"summary\"}, functions=functions)   | JsonOutputFunctionsParser()\n",
    "result = chain.invoke({\"topic\": \"Large Multimodal Model\"}, config={})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LCEL enables the creation of Retrieval-augmented generation chains, merging retrieval and language generation steps for a comprehensive and sophisticated approach to content creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Create a vector store and retriever\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\" ability of a digital computer or computer-controlled robot to perform tasks commonly associated with intelligent beings\",\"Artificially induced intelligence\"], embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Define templates for prompts\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retrieval-augmented generation chain.\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke(\"what is Artificial intelligence?\")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integration of Runnables allows for the concatenation of Multiple Chains, enabling a seamless connection between distinct processes for enhanced and cohesive text generation. In this instance, a branching and merging chain is applied to construct a rationale, analyze its merits and drawbacks, and then generate a conclusive response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 = ChatPromptTemplate.from_template(\"is this {city} caputal of this country?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\n",
    "    \"what country is the city {city} in? respond in {language}\"\n",
    ")\n",
    "\n",
    "model = ChatOpenAI()\n",
    "\n",
    "chain1 = prompt1 | model | StrOutputParser()\n",
    "\n",
    "result = chain1.invoke({\"city\": \"Rome\"})\n",
    "print(result)\n",
    "\n",
    "print(\"*\" * 40)\n",
    "\n",
    "chain2 = (\n",
    "    {\"city\": chain1, \"language\": itemgetter(\"language\")}\n",
    "    | prompt2\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain2.invoke({\"city\": \"Rome\", \"language\": \"spanish\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LCEL facilitates the splitting and merging of chains through RunnableMaps. Here’s an example illustrating **Branching and Merging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant 1 - Using RunnablePassthrough() that forwards the value to the input.\n",
    "\n",
    "planner = (\n",
    "    {\"input\": RunnablePassthrough()}\n",
    "    | ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    "    | {\"base_response\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "print(planner.invoke(\"Agile\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant 2 - Invoke based on the input. Use object with \"input\" key containing the query.\n",
    "\n",
    "planner = (\n",
    "    ChatPromptTemplate.from_template(\"Generate an argument about: {input}\")\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    "    | {\"base_response\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "print(planner.invoke({\"input\": \"Agile\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arguments_for = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the pros or positive aspects of {base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "arguments_against = (\n",
    "    ChatPromptTemplate.from_template(\n",
    "        \"List the cons or negative aspects of {base_response}\"\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_responder = (\n",
    "    ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"ai\", \"{original_response}\"),\n",
    "            (\"human\", \"Pros:\\n{results_1}\\n\\nCons:\\n{results_2}\"),\n",
    "            (\"system\", \"Generate a final response given the critique\"),\n",
    "        ]\n",
    "    )\n",
    "    | ChatOpenAI()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    planner\n",
    "    | {\n",
    "        \"results_1\": arguments_for,\n",
    "        \"results_2\": arguments_against,\n",
    "        \"original_response\": itemgetter(\"base_response\"),\n",
    "    }\n",
    "    | final_responder\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"input\": \"Agile\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runnable Batch, Stream & Async processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Batch processing** is enhanced through LangChain’s Expression Language, streamlining LLM queries by executing multiple tasks concurrently. LangChain’s batch functionality optimizes inputs by employing parallel LLM calls, ensuring efficient and improved performance in interactions with the LLM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"Given the items: {items}, What games can I play\")\n",
    "chain = prompt | model |  StrOutputParser()\n",
    "response = chain.batch([{\"items\": \"bat, ball, gloves\"},{\"items\":\"stick, ball, gloves, pads\"}])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stream functionality** in LangChain facilitates immediate data flow, making it well-suited for dynamic chatbots and live-stream applications. ChefBot exemplifies this capability by seamlessly streaming information, eliminating any wait time and showcasing the power of LangChain in dynamic contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"Given the items: {items}, What games can I play\")\n",
    "chain = prompt | model\n",
    "for s in chain.stream({\"items\": \"ball ,jersey, shoes\"}):print(s.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By leveraging ainvoke and await methods, a seamless asynchronous execution is achieved. This empowers tasks to operate independently, substantially enhancing responsiveness and application speed within the realm of asynchronous capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate.from_template(\"Given the items: {items}, What games can I play\")\n",
    "chain = prompt | model\n",
    "response = await chain.ainvoke({\"items\": \"shuttle cock, bat\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallelize steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RunnableParallel**, also known as RunnableMap, simplifies the simultaneous execution of multiple Runnables. It seamlessly returns the output of these Runnables as a map, providing an efficient and straightforward approach to parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "model = ChatOpenAI()\n",
    "story_chain = ChatPromptTemplate.from_template(\"tell me a story about {topic}\") | model\n",
    "poem_chain = ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\") | model\n",
    "\n",
    "map_chain = RunnableParallel(story=story_chain, poem=poem_chain)\n",
    "\n",
    "map_chain.invoke({\"topic\": \"goofy\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute custom functions ( Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **RunnableLambda**, within LangChain, serves as an abstraction enabling the transformation of custom Python functions into pipe functions.This example bears similarity to the Runnable class introduced earlier in the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "\n",
    "def add_two(x):\n",
    "    return x + 2\n",
    "\n",
    "# wrap the functions with RunnableLambda\n",
    "add_one = RunnableLambda(add_one)\n",
    "add_two = RunnableLambda(add_two)\n",
    "chain = add_one | add_two\n",
    "chain.invoke(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
