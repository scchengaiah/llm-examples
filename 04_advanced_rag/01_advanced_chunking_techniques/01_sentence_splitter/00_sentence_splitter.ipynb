{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character splitting poses an issue as it tends to cut sentences midway. Despite attempts to address this using chunk size and overlap, sentences can still be cut off prematurely. Let's explore a novel approach that considers sentence boundaries instead.\n",
    "\n",
    "The **SpacySentenceTokenizer** takes a piece of text and divides it into smaller chunks, with each chunk containing a certain number of sentences. It uses the Spacy library to analyze the input text and identify individual sentences.\n",
    "\n",
    "The method allows you to control the size of the chunks by specifying the stride and overlap parameters. *The stride determines how many sentences are skipped between consecutive chunks, and the overlap determines how many sentences from the previous chunk are included in the next one*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Direct Spacy Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "import spacy\n",
    "\n",
    "class SpacySentenceTokenizer:\n",
    "    def __init__(self, spacy_model=\"en_core_web_sm\"):\n",
    "        # Try loading the model\n",
    "        try:\n",
    "            self.nlp = spacy.load(spacy_model)\n",
    "        except OSError:\n",
    "            # If model is not found, download it and load again\n",
    "            print(f\"Downloading model {spacy_model}...\")\n",
    "            spacy.cli.download(spacy_model)\n",
    "            self.nlp = spacy.load(spacy_model)\n",
    "\n",
    "    def create_documents(self, documents, metadatas=None, overlap: int = 0, stride: int = 1) -> List[Document]:\n",
    "        chunks = []\n",
    "        if not metadatas:\n",
    "            metadatas = [{}]* len(documents)\n",
    "        for doc, metadata in zip(documents, metadatas):\n",
    "            text_chunks = self.split_text(doc, overlap, stride)\n",
    "            for chunk_text in text_chunks:\n",
    "                chunks.append(Document(page_content=chunk_text, metadata=metadata))\n",
    "        return chunks\n",
    "\n",
    "    def split_text(self, text: str, stride: int = 1, overlap: int = 1) -> List[str]:\n",
    "        sentences = list(self.nlp(text).sents)\n",
    "        chunks = []\n",
    "        for i in range(0, len(sentences), stride):\n",
    "            chunk_text = \" \".join(str(sent) for sent in sentences[i: i + overlap + 1])\n",
    "            chunks.append(chunk_text)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example below shows how a text with pronouns like ‚Äúthey‚Äù requires the context of the previous sentence to make sense. Our brute force overlap approach helps here but is also redundant at some places and leads to longer chunks üôÅ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love dogs. They are amazing. Cats must be the easiest pets around. Tesla robots are advanced now with AI. They will take us to mars.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Character splitting poses an issue as it tends to cut sentences midway. Despite attempts to address this using chunk size and overlap, sentences can still be cut off prematurely. Let's explore a novel approach that considers sentence boundaries instead.\n",
    "\n",
    "The **SpacySentenceTokenizer** takes a piece of text and divides it into smaller chunks, with each chunk containing a certain number of sentences. It uses the Spacy library to analyze the input text and identify individual sentences.\n",
    "\n",
    "The method allows you to control the size of the chunks by specifying the stride and overlap parameters. *The stride determines how many sentences are skipped between consecutive chunks, and the overlap determines how many sentences from the previous chunk are included in the next one*.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SpacySentenceTokenizer()\n",
    "splitted_text = tokenizer.split_text(text, stride=3, overlap=2)\n",
    "\n",
    "print(splitted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Langchain SpacyTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love dogs. They are amazing. Cats must be the easiest pets around. Tesla robots are advanced now with AI. They will take us to mars.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=500)\n",
    "\n",
    "texts = text_splitter.split_text(text)\n",
    "for text_chunk in texts:\n",
    "    print(\"*\" * 50)\n",
    "    print(text_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"../docs/Intelizign Leave Policy.pdf\")\n",
    "pages = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import SpacyTextSplitter\n",
    "\n",
    "text_splitter = SpacyTextSplitter(chunk_size=500)\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "if True:\n",
    "    for doc in docs:\n",
    "        print(\"*\" * 50)\n",
    "        print(\"METADATA:\")\n",
    "        print(doc.metadata)\n",
    "        print(\"CONTENT:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"*\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
