{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Based Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Propositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the conventional use of passages or sentences, a new paper [Dense X Retrieval: What Retrieval Granularity Should We Use?](https://chentong0.github.io/factoid-wiki/) introduces a novel retrieval unit for dense retrieval called \"propositions.\" Propositions are atomic expressions within text, each encapsulating a distinct factoid and presented in a concise, self-contained natural language format.\n",
    "\n",
    "The three principles below define propositions as atomic expressions of meanings in text:\n",
    "\n",
    "* Each proposition should represent a distinct piece of meaning in the text, collectively embodying the semantics of the entire text.\n",
    "* A proposition must be minimal and cannot be further divided into separate propositions.\n",
    "* A proposition should contextualize itself and be self-contained, encompassing all the necessary context from the text (e.g., coreference) to interpret its meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging Opensource Propositioner model using Flan T5 architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "import json\n",
    "\n",
    "model_name = \"chentong00/propositionizer-wiki-flan-t5-large\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Setting up use_fast=False due to this (https://github.com/huggingface/transformers/releases/tag/v4.0.0)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"General Information\"\n",
    "section = \"\"\n",
    "content = \"Cats love dogs. Think They are amazing. Dogs must be the easiest pets around. Tesla robots are advanced now with AI. They will take us to mars.\"\n",
    "\n",
    "input_text = f\"Title: {title}. Section: {section}. Content: {content}\"\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids.to(device), max_new_tokens=512).cpu()\n",
    "\n",
    "output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-vector indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach involves multi-vector indexing, where semantic search is performed for a vector derived from something other than the raw text. There are various methods to create multiple vectors per document.\n",
    "\n",
    "\n",
    "**Smaller chunks:**\n",
    "\n",
    "Divide a document into smaller chunks and embed them (referred to as ParentDocumentRetriever).\n",
    "\n",
    "**Summary:**\n",
    "\n",
    "Generate a summary for each document and embed it along with, or instead of, the document.\n",
    "\n",
    "**Hypothetical questions:**\n",
    "\n",
    "Form hypothetical questions that each document would be appropriate to answer, and embed them along with, or instead of, the document.\n",
    "\n",
    "Each of these utilizes either a text2text or an LLM with a prompt to obtain the necessary chunk. The system then indexes both the newly generated chunk and the original text, improving the recall of the retrieval system. You can find more details of these techniques in Langchainâ€™s official [documentation](https://python.langchain.com/docs/how_to/multi_vector/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
