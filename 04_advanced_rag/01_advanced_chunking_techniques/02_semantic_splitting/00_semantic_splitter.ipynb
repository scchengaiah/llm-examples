{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **SimilarSentenceSplitter** takes a piece of text and divides it into groups of sentences based on their similarity. It utilizes a similarity model to measure how similar each sentence is to its neighboring sentences. The method uses a sentence splitter to break the input text into individual sentences.\n",
    "\n",
    "The goal is to create groups of sentences where each group contains related sentences, according to the specified similarity model. The method starts with the first sentence in the first group and then iterates through the remaining sentences. It decides whether to add a sentence to the current group based on its similarity to the previous sentence.\n",
    "\n",
    "The **group_max_sentences** parameter controls the maximum number of sentences allowed in each group. If a group reaches this limit, a new group is started. Additionally, a new group is initiated if the similarity between consecutive sentences falls below a specified similarity_threshold.\n",
    "\n",
    "In simpler terms, this method organizes a text into clusters of sentences, where sentences within each cluster are considered similar to each other. It's useful for identifying coherent and related chunks of information within a larger body of text.\n",
    "\n",
    "\n",
    "**Related Repo:** [https://github.com/agamm/semantic-split](https://github.com/agamm/semantic-split) - Downloaded local copy as well [here](../02_semantic_splitting/). However, we are using packaged version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Using Semantic split module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_split import SimilarSentenceSplitter, SentenceTransformersSimilarity, SpacySentenceSplitter\n",
    "\n",
    "text = \"\"\"\n",
    "  I dogs are amazing.\n",
    "  Cats must be the easiest pets around.\n",
    "  Lion is a ferocious animal.\n",
    "  Rose is a beautiful flower.\n",
    "  Robots are advanced now with AI.\n",
    "  Flying in space can only be done by Artificial intelligence.\"\"\"\n",
    "\n",
    "model = SentenceTransformersSimilarity()\n",
    "sentence_splitter = SpacySentenceSplitter()\n",
    "splitter = SimilarSentenceSplitter(model, sentence_splitter)\n",
    "res = splitter.split(text)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use partial langchain implementation to load pdf and setup an implementation to leverage opensource option as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"../docs/Intelizign Leave Policy.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "pages[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "def create_documents(texts: List[str], metadatas: Optional[List[dict]] = None) -> List[str]:\n",
    "        semantic_chunks = []\n",
    "        for i, text in enumerate(texts):\n",
    "            start_index = 0\n",
    "            for chunk in splitter.split(text):\n",
    "                semantic_chunks.append(chunk)\n",
    "        return semantic_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the chunks\n",
    "def embed_chunks(chunks: List[str]) -> List[List[float]]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, metadatas = [], []\n",
    "for doc in pages:\n",
    "    texts.append(doc.page_content)\n",
    "    metadatas.append(doc.metadata)\n",
    "\n",
    "semantic_chunks = create_documents(texts, metadatas=metadatas)\n",
    "print(semantic_chunks[10:20])\n",
    "\n",
    "# Create embeddings of the contextually relevant sentences.\n",
    "embedded_chunks = [embed_chunks(item) for item in semantic_chunks]\n",
    "print(embedded_chunks[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Langchain Semantic Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "env_loaded = load_dotenv(\".env\")\n",
    "\n",
    "print(f\"Env loaded: {env_loaded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"../docs/Intelizign Leave Policy.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "pages[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "azure_openai_embeddings = AzureOpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
    "\n",
    "embedding_arr = azure_openai_embeddings.embed_query(text = \"This is a test call.\")\n",
    "\n",
    "embedding_arr[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_text_splitter = SemanticChunker(azure_openai_embeddings)\n",
    "\n",
    "docs = semantic_text_splitter.split_documents(pages)\n",
    "\n",
    "if True:\n",
    "    for doc in docs:\n",
    "        print(\"*\" * 50)\n",
    "        print(\"METADATA:\")\n",
    "        print(doc.metadata)\n",
    "        print(\"CONTENT:\")\n",
    "        print(doc.page_content)\n",
    "        print(\"*\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several options to configure breakpoint thresold type `PERCENTILE`, `STANDARD_DEVIATION`, `INTERQUARTILE` and `GRADIENT`.\n",
    "\n",
    "Refer to the documentation [here](https://python.langchain.com/docs/how_to/semantic-chunker/#breakpoints) for detailed information along with the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
