{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiModal RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below implementation is developed based on this [medium article](https://medium.com/artificial-corner/multimodal-retrieval-augmented-generation-for-sustainable-finance-with-code-5a910f3b666c).\n",
    "\n",
    "The notebook example of this complete article can be found [here](./references/00_multimodal-rag-esg-main/notebooks/ESG_Multimodal_RAG_v2.ipynb) that contains multiple modalities such as video to audio transcriptions, images, tables and text. However, most of our enterprise usecases deals with images, tables and text within the pdf, hence, we have created this version of notebook to simplify the implementation for further reference.\n",
    "\n",
    "Also, we shall use pgvector as a vector database instead of weaviate which was used in the article.\n",
    "\n",
    "This is a minimalistic example that can be used as getting started to understand the fundamentals. However, this does not consider the complete contents of the PDF during the chunking process.\n",
    "\n",
    "Refer to [README.md](./README.md) for more references. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Unstructured Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unstructured[pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Parse PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_report_path = \"./data/Global_ESG_Q1_2024_Flows_Report.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "esg_report_raw_data =partition_pdf(\n",
    "    filename=esg_report_path,\n",
    "    strategy=\"hi_res\",\n",
    "    extract_images_in_pdf=True,\n",
    "    extract_image_block_to_payload=False,\n",
    "    extract_image_block_output_dir=\"./data/images/\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esg_report_raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract Textual Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may not extract the entire content since some of the elements generated from unstructured falls into other categories such as `ListItem`, `Title`, etc..\n",
    "\n",
    "If we want to consider the whole text, we can go with other types of parsers such as `pymupdf4llm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.documents.elements import NarrativeText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_metadata(esg_report, source_document):\n",
    "\n",
    "    text_data = []\n",
    "    paragraph_counters = {}\n",
    "\n",
    "    for element in esg_report:\n",
    "        if isinstance(element, NarrativeText):\n",
    "            page_number = element.metadata.page_number\n",
    "\n",
    "            if page_number not in paragraph_counters:\n",
    "                paragraph_counters[page_number] = 1\n",
    "            else:\n",
    "                paragraph_counters[page_number] += 1\n",
    "\n",
    "            paragraph_number = paragraph_counters[page_number]\n",
    "\n",
    "            text_content = element.text\n",
    "            text_data.append({\n",
    "                \"source_document\": source_document,\n",
    "                \"page_number\": page_number,\n",
    "                \"paragraph_number\": paragraph_number,\n",
    "                \"text\": text_content\n",
    "            })\n",
    "\n",
    "    return text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = extract_text_with_metadata(esg_report_raw_data, esg_report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract Image components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.documents.elements import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_metadata(esg_report, source_document):\n",
    "    image_data = []\n",
    "\n",
    "    for element in esg_report:\n",
    "        if isinstance(element, Image):\n",
    "            page_number = element.metadata.page_number\n",
    "            image_path = element.metadata.image_path if hasattr(element.metadata, 'image_path') else None\n",
    "\n",
    "            image_data.append({\n",
    "                \"source_document\": source_document,\n",
    "                \"page_number\": page_number,\n",
    "                \"image_path\": image_path\n",
    "            })\n",
    "\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_image_data = extract_image_metadata(esg_report_raw_data, esg_report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_from_metadata(extracted_image_data, images_per_row=4):\n",
    "    valid_images = [img for img in extracted_image_data if img['image_path']]\n",
    "    if not valid_images:\n",
    "        print(\"No valid image data available.\")\n",
    "        return\n",
    "\n",
    "    num_images = len(valid_images)\n",
    "    num_rows = math.ceil(num_images / images_per_row)\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, images_per_row, figsize=(20, 5*num_rows))\n",
    "    axes = axes.flatten() if num_rows > 1 else [axes]\n",
    "\n",
    "    for ax, img_data in zip(axes, valid_images):\n",
    "        try:\n",
    "            img = Image.open(img_data['image_path'])\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(f\"Page {img_data['page_number']}\", fontsize=10)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_data['image_path']}: {str(e)}\")\n",
    "            ax.text(0.5, 0.5, f\"Error loading image\\n{str(e)}\", ha='center', va='center')\n",
    "            ax.axis('off')\n",
    "\n",
    "    for ax in axes[num_images:]:\n",
    "        fig.delaxes(ax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images_from_metadata(extracted_image_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract Table Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.documents.elements import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_metadata(esg_report, source_document):\n",
    "    table_data = []\n",
    "\n",
    "    for element in esg_report:\n",
    "        if isinstance(element, Table):\n",
    "            page_number = element.metadata.page_number\n",
    "\n",
    "            # Extract table content as a string\n",
    "            table_content = str(element)\n",
    "\n",
    "            table_data.append({\n",
    "                \"source_document\": source_document,\n",
    "                \"page_number\": page_number,\n",
    "                \"table_content\": table_content\n",
    "            })\n",
    "\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_table_data = extract_table_metadata(esg_report_raw_data, esg_report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Image and Table Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images and Tables will be described in a way that make them undertandable in a few sentences.\n",
    "\n",
    "For both image and table, we get a description first using the corresponding prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Table summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-core\n",
    "!pip install langchain-openai\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_summarizer_prompt = \"\"\"\n",
    "As an ESG analyst for emerging markets investments, provide a concise and exact summary of the table contents.\n",
    "Focus on key ESG metrics (Environmental, Social, Governance) and their relevance to emerging markets.\n",
    "Highlight significant trends, comparisons, or outliers in the data. Identify any potential impacts on investment strategies or risk assessments.\n",
    "Avoid bullet points; instead, deliver a coherent, factual summary that captures the essence of the table for ESG investment decision-making.\n",
    "\n",
    "Table: {table_content}\n",
    "\n",
    "Limit your summary to 3-4 sentences, ensuring it's precise and informative for ESG analysis in emerging markets.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_model = AzureChatOpenAI(model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_table_metadata_with_summary(esg_report,\n",
    "                                        source_document,\n",
    "                                        tables_summarizer_prompt):\n",
    "\n",
    "    table_data = []\n",
    "    prompt = ChatPromptTemplate.from_template(tables_summarizer_prompt)\n",
    "\n",
    "    for element in esg_report:\n",
    "        if isinstance(element, Table):\n",
    "            page_number = element.metadata.page_number\n",
    "\n",
    "            # Extract table content as a string\n",
    "            table_content = str(element)\n",
    "\n",
    "            # Generate summary using the OpenAI model\n",
    "            messages = prompt.format_messages(table_content=table_content)\n",
    "            description = description_model.invoke(messages).content\n",
    "\n",
    "            table_data.append({\n",
    "                \"source_document\": source_document,\n",
    "                \"page_number\": page_number,\n",
    "                \"table_content\": table_content,\n",
    "                \"description\": description\n",
    "            })\n",
    "\n",
    "    return table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_table_data_with_summary = extract_table_metadata_with_summary(esg_report_raw_data,\n",
    "                                                                        esg_report_path,\n",
    "                                                                        tables_summarizer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_table_data_with_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first key-value pair in the dictionary\n",
    "first_table_details = extracted_table_data_with_summary[0]\n",
    "\n",
    "# Extract the transcription from the first item\n",
    "first_description = first_table_details\n",
    "\n",
    "first_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_description['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Image Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "from langchain_core.messages import HumanMessage\n",
    "import base64\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_summarizer_prompt = \"\"\"\n",
    "As an ESG analyst for emerging markets investments, please provide a clear interpretation of data or information that see describe from the image.\n",
    "Focus on ESG-relevant content (Environmental, Social, Governance) and any emerging market context. Describe the type of visual (e.g., chart, photograph, infographic) and its key elements.\n",
    "Highlight significant data points or trends that are relevant to investment analysis. Avoid bullet points; instead, deliver a coherent, factual summary that captures the essence of the image for ESG investment decision-making.\n",
    "\n",
    "Ground your response based on the provided image and do not hallucinate.\n",
    "\n",
    "Limit your description to 3-4 sentences, ensuring it's precise and informative for ESG analysis.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_metadata_with_summary(esg_report_raw_data,\n",
    "                                        esg_report_path,\n",
    "                                        images_summarizer_prompt):\n",
    "\n",
    "    image_data = []\n",
    "\n",
    "    # Create ChatPromptTemplate instance\n",
    "    prompt = ChatPromptTemplate.from_template(images_summarizer_prompt)\n",
    "\n",
    "    # Create ChatOpenAI instance\n",
    "    description_model = AzureChatOpenAI(model=model_id)\n",
    "\n",
    "    for element in esg_report_raw_data:\n",
    "        if \"Image\" in str(type(element)):\n",
    "            page_number = element.metadata.page_number if hasattr(element.metadata, 'page_number') else None\n",
    "            image_path = element.metadata.image_path if hasattr(element.metadata, 'image_path') else None\n",
    "\n",
    "            # Read the image file and encode it to base64\n",
    "            with open(image_path, \"rb\") as image_file:\n",
    "                encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "\n",
    "            if image_path and os.path.exists(image_path):\n",
    "                # Generate description using the OpenAI model\n",
    "                messages = HumanMessage(\n",
    "                    content=[\n",
    "                        {\"type\": \"text\", \"text\": images_summarizer_prompt},\n",
    "                        {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\"url\": f\"data:image/jpeg;base64,{encoded_string}\"},\n",
    "                        },\n",
    "                    ],\n",
    "                )\n",
    "                description = description_model.invoke([messages]).content\n",
    "\n",
    "                \n",
    "                image_data.append({\n",
    "                    \"source_document\": esg_report_path,\n",
    "                    \"page_number\": page_number,\n",
    "                    \"image_path\": image_path,\n",
    "                    \"description\": description,\n",
    "                    \"base64_encoding\": encoded_string\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: Image file not found or path not available for image on page {page_number}\")\n",
    "\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_image_data = extract_image_metadata_with_summary(esg_report_raw_data,\n",
    "                                                           esg_report_path,\n",
    "                                                           images_summarizer_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first key-value pair in the dictionary\n",
    "sixth_image_details = extracted_image_data[5]\n",
    "\n",
    "sixth_image_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sixth_image_details['description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Upload - Pgvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection parameters\n",
    "db_params = {\n",
    "    \"dbname\": \"pgvector-exploration\",\n",
    "    \"user\": \"admin\",\n",
    "    \"password\": \"admin\",\n",
    "    \"host\": \"172.31.60.199\",  # Use the appropriate host\n",
    "    \"port\": \"15432\"        # Default PostgreSQL port\n",
    "}\n",
    "\n",
    "# Connect to the PostgreSQL database\n",
    "with psycopg.connect(**db_params) as conn:\n",
    "    print(\"Postgresql Test connection successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorstore Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = \"postgresql+psycopg://admin:admin@172.31.60.199:15432/pgvector-exploration\" \n",
    "collection_name = \"esg_reports\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings( model=\"text-embedding-3-small\", api_version=\"2024-02-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get embeddings\n",
    "def get_embedding(text):\n",
    "    response = embeddings.embed_query(text)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop the tables created by the vectorstore (e.g., updating the embedding to a different dimension or just updating the embedding provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.drop_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from tqdm import tqdm\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_template = {\n",
    "    \"id\": None,\n",
    "    \"source_document\": None,\n",
    "    \"page_number\": None,\n",
    "    \"paragraph_number\": None,\n",
    "    \"image_path\": None,\n",
    "    \"base64_encoding\": None,\n",
    "    \"table_content\": None,\n",
    "    \"content_type\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_text_data(text_data):\n",
    "    docs = []\n",
    "    for text in tqdm(text_data, desc=\"Ingesting text data\"):\n",
    "        metadata = metadata_template.copy()\n",
    "        metadata[\"id\"] = str(uuid.uuid4())\n",
    "        metadata[\"source_document\"] = text['source_document']\n",
    "        metadata[\"page_number\"] = text['page_number']\n",
    "        metadata[\"paragraph_number\"] = text['paragraph_number']\n",
    "        metadata[\"content_type\"] = \"text\"\n",
    "        \n",
    "        # Instantiate Document Object and append to the list\n",
    "        docs.append(Document(page_content=text['text'], metadata=metadata))\n",
    "    \n",
    "    vectorstore.add_documents(docs, ids=[doc.metadata['id'] for doc in docs])\n",
    "\n",
    "\n",
    "def ingest_image_data(image_data):\n",
    "    docs = []\n",
    "    for image in tqdm(image_data, desc=\"Ingesting image data\"):\n",
    "        metadata = metadata_template.copy()\n",
    "        metadata[\"id\"] = str(uuid.uuid4())\n",
    "        metadata[\"source_document\"] = image['source_document']\n",
    "        metadata[\"page_number\"] = image['page_number']\n",
    "        metadata[\"image_path\"] = image['image_path']\n",
    "        metadata[\"base64_encoding\"] = image['base64_encoding']\n",
    "        metadata[\"content_type\"] = \"image\"\n",
    "        \n",
    "        # Instantiate Document Object and append to the list\n",
    "        docs.append(Document(page_content=image['description'], metadata=metadata))\n",
    "    \n",
    "    vectorstore.add_documents(docs, ids=[doc.metadata['id'] for doc in docs])\n",
    "\n",
    "def ingest_table_data(table_data):\n",
    "    docs = []\n",
    "    for table in tqdm(table_data, desc=\"Ingesting table data\"):\n",
    "        metadata = metadata_template.copy()\n",
    "        metadata[\"id\"] = str(uuid.uuid4())\n",
    "        metadata[\"source_document\"] = table['source_document']\n",
    "        metadata[\"page_number\"] = table['page_number']\n",
    "        metadata[\"table_content\"] = table['table_content']\n",
    "        metadata[\"content_type\"] = \"table\"\n",
    "        \n",
    "        # Instantiate Document Object and append to the list\n",
    "        docs.append(Document(page_content=table['description'], metadata=metadata))\n",
    "    \n",
    "    vectorstore.add_documents(docs, ids=[doc.metadata['id'] for doc in docs])\n",
    "\n",
    "def ingest_all_data(text_data, image_data, table_data):\n",
    "    ingest_text_data(text_data)\n",
    "    ingest_image_data(image_data)\n",
    "    ingest_table_data(table_data)\n",
    "    print(\"All objects imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_all_data(text_data=extracted_data,\n",
    "                image_data=extracted_image_data,\n",
    "                table_data=extracted_table_data_with_summary\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query PgVector for Most Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_multimodal(query: str, limit: int = 3):\n",
    "    retriever = vectorstore.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\": limit})\n",
    "    return retriever.invoke(query)\n",
    "\n",
    "def search_multimodal_with_score(query: str, limit: int = 3):\n",
    "    docs_with_score = vectorstore.similarity_search_with_score(query, k=limit)\n",
    "    return docs_with_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_print_results(query, limit=3):\n",
    "\n",
    "    search_results = search_multimodal(query, limit)\n",
    "\n",
    "    print(f\"Search Results for query: '{query}'\")\n",
    "    for item in search_results:\n",
    "        print(f\"Type: {item.metadata['content_type']}\")\n",
    "        if item.metadata['content_type'] == 'text':\n",
    "            print(f\"Source: {item.metadata['source_document']}, Page: {item.metadata['page_number']}\")\n",
    "            print(f\"Paragraph {item.metadata['paragraph_number']}\")\n",
    "            print(f\"Text: {item.page_content[:100]}...\")\n",
    "        elif item.metadata['content_type'] == 'image':\n",
    "            print(f\"Source: {item.metadata['source_document']}, Page: {item.metadata['page_number']}\")\n",
    "            print(f\"Image Source: {item.metadata['image_path']}, Page: {item.metadata['page_number']}\")\n",
    "            print(f\"Description: {item.page_content}\")\n",
    "        elif item.metadata['content_type'] == 'table':\n",
    "            print(f\"Source: {item.metadata['source_document']}, Page: {item.metadata['page_number']}\")\n",
    "            print(f\"Description: {item.page_content}\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_print_results_with_score(query, limit=3):\n",
    "\n",
    "    search_results = search_multimodal_with_score(query, limit)\n",
    "\n",
    "    print(f\"Search Results for query: '{query}'\")\n",
    "    for item, score in search_results:\n",
    "        print(f\"Type: {item.metadata['content_type']}\")\n",
    "        print(f\"Cosine Similarity: {1-score}\")\n",
    "        if item.metadata['content_type'] == 'text':\n",
    "            print(f\"Source: {item.metadata['source_document']}, Page: {item.metadata['page_number']}\")\n",
    "            print(f\"Paragraph {item.metadata['paragraph_number']}\")\n",
    "            print(f\"Text: {item.page_content[:100]}...\")\n",
    "        elif item.metadata['content_type'] == 'image':\n",
    "            print(f\"Source: {item.metadata['source_document']}, Page: {item.metadata['page_number']}\")\n",
    "            print(f\"Image Source: {item.metadata['image_path']}, Page: {item.metadata['page_number']}\")\n",
    "            print(f\"Description: {item.page_content}\")\n",
    "        elif item.metadata['content_type'] == 'table':\n",
    "            print(f\"Source: {item.metadata['source_document']}, Page: {item.metadata['page_number']}\")\n",
    "            print(f\"Description: {item.page_content}\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main environmental challenges in renewable energy?\"\n",
    "search_and_print_results_with_score(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal RAG for ESG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = AzureChatOpenAI(model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query: str, context: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant specializing in ESG (Environmental, Social, and Governance) analysis for emerging markets.\n",
    "    Use the following pieces of information to answer the user's question.\n",
    "    If you cannot answer the question based on the provided information, say that you don't have enough information to answer accurately.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    User Question: {query}\n",
    "\n",
    "    Please provide a detailed and accurate answer based on the given context:\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "    messages = prompt.format_messages(query=query, context=context)\n",
    "\n",
    "    response = chat_model.invoke(messages)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esg_analysis(user_query: str):\n",
    "\n",
    "    # Step 1: Retrieve relevant information\n",
    "    search_results = search_multimodal_with_score(user_query)\n",
    "\n",
    "    # Step 2: Prepare context for RAG\n",
    "    context = \"\"\n",
    "    for item, score in search_results:\n",
    "        if item.metadata['content_type'] == 'text':\n",
    "            context += f\"Text from {item.metadata['source_document']} (Page {item.metadata['page_number']}, Paragraph {item.metadata['paragraph_number']}): {item.page_content}\\n\\n\"\n",
    "        elif item.metadata['content_type'] == 'image':\n",
    "            context += f\"Image Description from {item.metadata['source_document']} (Page {item.metadata['page_number']}, Path: {item.metadata['image_path']}): {item.page_content}\\n\\n\"\n",
    "        elif item.metadata['content_type'] == 'table':\n",
    "            context += f\"Table Description from {item.metadata['source_document']} (Page {item.metadata['page_number']}): {item.page_content}\\n\\n\"\n",
    "\n",
    "    # Step 3: Generate response using RAG\n",
    "    response = generate_response(user_query, context)\n",
    "\n",
    "    # Step 4: Format and return the final output\n",
    "    sources = []\n",
    "    for item, score in search_results:\n",
    "        source = {\n",
    "            \"type\": item.metadata[\"content_type\"],\n",
    "            \"distance\": score\n",
    "        }\n",
    "        if item.metadata[\"content_type\"] == 'text':\n",
    "            source.update({\n",
    "                \"document\": item.metadata[\"source_document\"],\n",
    "                \"page\": item.metadata[\"page_number\"],\n",
    "                \"paragraph\": item.metadata[\"paragraph_number\"]\n",
    "            })\n",
    "        elif item.metadata[\"content_type\"] == 'image':\n",
    "            source.update({\n",
    "                \"document\": item.metadata[\"source_document\"],\n",
    "                \"page\": item.metadata[\"page_number\"],\n",
    "                \"image_path\": item.metadata[\"image_path\"]\n",
    "            })\n",
    "        elif item.metadata[\"content_type\"] == 'table':\n",
    "            source.update({\n",
    "                \"document\": item.metadata[\"source_document\"],\n",
    "                \"page\": item.metadata[\"page_number\"]\n",
    "            })\n",
    "        \n",
    "        sources.append(source)\n",
    "\n",
    "    # Sort sources by distance (ascending order)\n",
    "    sources.sort(key=lambda x: x['distance'])\n",
    "\n",
    "    final_output = {\n",
    "        \"user_query\": user_query,\n",
    "        \"ai_response\": response,\n",
    "        \"sources\": sources\n",
    "    }\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def wrap_text(text, width=120):\n",
    "    wrapped_text = textwrap.fill(text, width=width)\n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_and_print_esg_results(user_question):\n",
    "    result = esg_analysis(user_question)\n",
    "\n",
    "    print(\"User Query:\", result[\"user_query\"])\n",
    "    print(\"\\nAI Response:\", wrap_text(result[\"ai_response\"]))\n",
    "    print(\"\\nSources (sorted by relevance):\")\n",
    "    for source in result[\"sources\"]:\n",
    "        print(f\"- Type: {source['type']}, Distance: {source['distance']:.3f}\")\n",
    "        if source['type'] == 'text':\n",
    "            print(f\"  Document: {source['document']}, Page: {source['page']}, Paragraph: {source['paragraph']}\")\n",
    "        elif source['type'] == 'image':\n",
    "            print(f\"  Document: {source['document']}, Page: {source['page']}, Image Path: {source['image_path']}\")\n",
    "        elif source['type'] == 'table':\n",
    "            print(f\"  Document: {source['document']}, Page: {source['page']}\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"Is ESG investment a fraud?\"\n",
    "analyze_and_print_esg_results(user_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
