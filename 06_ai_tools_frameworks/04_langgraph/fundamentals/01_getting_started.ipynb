{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is a getting started section explored with the help of the blog from Medium\n",
    "\n",
    "https://medium.com/towards-data-science/from-basics-to-advanced-exploring-langgraph-e8c1cf4db787"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "import psycopg\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_loaded = load_dotenv()\n",
    "print(f\"Env loaded: {env_loaded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global connection object (initialized by helper function)\n",
    "_conn = None\n",
    "\n",
    "def _get_db_connection():\n",
    "    \"\"\"Helper function to get a database connection.\"\"\"\n",
    "    global _conn\n",
    "    db_host = os.environ.get(\"POSTGRES_HOST\")\n",
    "    db_user = os.environ.get(\"POSTGRES_USER\")\n",
    "    db_password = os.environ.get(\"POSTGRES_PASSWORD\")\n",
    "    db_name = os.environ.get(\"POSTGRES_DB\")\n",
    "    db_port = os.environ.get(\"POSTGRES_PORT\")\n",
    "\n",
    "    connection_string = (\n",
    "        f\"postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "    )\n",
    "    print(connection_string)\n",
    "    if _conn is None or _conn.closed:\n",
    "        try:\n",
    "            _conn = psycopg.connect(connection_string)\n",
    "            _conn.autocommit = False  # Set autocommit to False for more control\n",
    "        except (Exception, psycopg.Error) as error:\n",
    "            print(traceback.format_exc())\n",
    "            raise Exception(f\"Error connecting to database: {error}\")\n",
    "\n",
    "    return _conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sql_data(query):\n",
    "    \"\"\"\n",
    "    Executes the SQL query passed as an argument and returns the data from\n",
    "    Postgresql database in nicely formatted textual format.\n",
    "\n",
    "    Args:\n",
    "        query (str): The SQL query to execute.\n",
    "\n",
    "    Returns:\n",
    "        str: A nicely formatted string representation of the data returned\n",
    "             by the query.\n",
    "             Returns \"Error: No results found\" if the query returns no data or\n",
    "             an error message if an exception occurs.\n",
    "    \"\"\"\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = _get_db_connection()\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(query)\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "        if not rows:\n",
    "            return \"Error: No results found\"\n",
    "\n",
    "        # Get column names for headers\n",
    "        column_names = [desc[0] for desc in cur.description]\n",
    "\n",
    "        # Format data with headers\n",
    "        formatted_data = \"\"\n",
    "\n",
    "        # Calculate maximum width for each column\n",
    "        max_widths = [len(str(col)) for col in column_names]\n",
    "        for row in rows:\n",
    "            for i, value in enumerate(row):\n",
    "                max_widths[i] = max(max_widths[i], len(str(value)))\n",
    "\n",
    "        # Create header line\n",
    "        header_line = \"|\"\n",
    "        for i, col in enumerate(column_names):\n",
    "            header_line += f\" {col.ljust(max_widths[i])} |\"\n",
    "        formatted_data += header_line + \"\\n\"\n",
    "\n",
    "        # Create separator line\n",
    "        separator_line = \"|\"\n",
    "        for width in max_widths:\n",
    "            separator_line += f\"-{'-'*width}-|\"\n",
    "        formatted_data += separator_line + \"\\n\"\n",
    "\n",
    "        # Create data lines\n",
    "        for row in rows:\n",
    "            row_line = \"|\"\n",
    "            for i, value in enumerate(row):\n",
    "                row_line += f\" {str(value).ljust(max_widths[i])} |\"\n",
    "            formatted_data += row_line + \"\\n\"\n",
    "\n",
    "        cur.close()\n",
    "        return formatted_data\n",
    "\n",
    "    except (Exception, psycopg.Error) as error:\n",
    "        return f\"Database returned error: {error}\"\n",
    "    finally:\n",
    "        if conn:  # if connection was established\n",
    "            # Do not close the global connection object\n",
    "            # conn.close()\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "query1 = \"SELECT * FROM cate_ewa.pcb_labels;\"\n",
    "\n",
    "print(\"Query 1 Result:\\n\", get_sql_data(query1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s define one tool named execute_sql , which enables the execution of any SQL query. We use pydantic to specify the tool’s structure, ensuring that the LLM agent has all the needed information to use the tool effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class SQLQuery(BaseModel):\n",
    "    query: str = Field(description=\"SQL query to execute\")\n",
    "\n",
    "\n",
    "@tool(args_schema=SQLQuery)\n",
    "def execute_sql(query: str) -> str:\n",
    "    \"\"\"Returns the result of SQL query execution\"\"\"\n",
    "    return get_sql_data(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the parameters of the created tool to see what information is passed to LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"\"\"\n",
    "name: {execute_sql.name}\n",
    "description: {execute_sql.description}\n",
    "arguments: {execute_sql.to_json()}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current example is relatively straightforward. So, we will only need to store the history of messages. Let’s define the agent state.\n",
    "\n",
    "We’ve defined a single parameter in AgentState — messages — which is a list of objects of the class AnyMessage . Additionally, we annotated it with operator.add (reducer). This annotation ensures that each time a node returns a message, it is appended to the existing list in the state. Without this operator, each new message would replace the previous value rather than being added to the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "# defining agent state\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to define the agent itself. Let’s start with __init__ function. We will specify three arguments for the agent: model, list of tools and system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQLAgent:\n",
    "    def __init__(self, model, tools, system_prompt=\"\"):\n",
    "        self.system_prompt = system_prompt\n",
    "\n",
    "        # initialising graph with a state\n",
    "        graph = StateGraph(AgentState)\n",
    "\n",
    "        # adding nodes\n",
    "        graph.add_node(\"llm\", self.call_llm)\n",
    "        graph.add_node(\"function\", self.execute_function)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\", self.exists_function_calling, {True: \"function\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"function\", \"llm\")\n",
    "\n",
    "        # setting starting point\n",
    "        graph.set_entry_point(\"llm\")\n",
    "\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_function_calling(self, state: AgentState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def call_llm(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system_prompt:\n",
    "            messages = [SystemMessage(content=self.system_prompt)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {\"messages\": [message]}\n",
    "\n",
    "    def execute_function(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t[\"name\"] in self.tools:  # check for bad tool name from LLM\n",
    "                print(\"\\n ....bad tool name....\")\n",
    "                result = \"bad tool name, retry\"  # instruct LLM to retry if bad\n",
    "            else:\n",
    "                result = self.tools[t[\"name\"]].invoke(t[\"args\"])\n",
    "            results.append(\n",
    "                ToolMessage(tool_call_id=t[\"id\"], name=t[\"name\"], content=str(result))\n",
    "            )\n",
    "        print(\"Back to the model!\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "\n",
    "# system prompt\n",
    "prompt = \"\"\"You are a senior expert in SQL and data analysis. \n",
    "So, you can help the team to gather needed data to power their decisions. \n",
    "You are very accurate and take into account all the nuances in data.\n",
    "Your goal is to provide the detailed documentation for the table in database \n",
    "that will help users.\"\"\"\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=0,\n",
    "    max_tokens=8192,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_agent = SQLAgent(model, [execute_sql], system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph provides us with quite a handy feature to visualise graphs. To use it, you need to install pygraphviz.\n",
    "\n",
    "Refer to the [requirements.txt](../requirements.txt) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(doc_agent.graph.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ask a query that lets the agent to use the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Question\n",
    "messages = [HumanMessage(content=\"Can you list down all the schemas in my database ?\")]\n",
    "\n",
    "# Complicated Question\n",
    "# messages = [HumanMessage(content=\"Can you list down all the schemas in my database ? Along with that, List down all the tables present in each of these schemas.\")]\n",
    "\n",
    "result = doc_agent.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ask a query that is general and does not involve a tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Question\n",
    "messages = [HumanMessage(content=\"What is python ? Is it a popular programming language. Explain within 50 words.\")]\n",
    "\n",
    "result = doc_agent.graph.invoke({\"messages\": messages})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the result variable, we can observe all the messages generated during execution.\n",
    "\n",
    "We can observe that the agent has used the tool to fetch the schema information. If the question is general, then the tool call will not happen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"messages\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the final result. It looks pretty decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Prebuilt Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "prebuilt_doc_agent = create_react_agent(model, [execute_sql], state_modifier=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(prebuilt_doc_agent.get_graph().draw_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"messages\": [(\"user\", \"What columns are in airflow_test.dag_runs table?\")]}\n",
    "\n",
    "result = prebuilt_doc_agent.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stream(stream):\n",
    "    for s in stream:\n",
    "        message = s[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stream(prebuilt_doc_agent.stream(inputs, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence and streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us in memory context saving using MemorySaver to provide context to the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prebuilt_doc_agent = create_react_agent(model, [execute_sql], checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"18\"}}\n",
    "messages = [\n",
    "    HumanMessage(content=\"What info do we have in airflow_test.dag_runs table?\")\n",
    "]\n",
    "\n",
    "for event in prebuilt_doc_agent.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        v[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same thread and the agent has the possibility to understand the context from previous invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "followup_messages = [\n",
    "    HumanMessage(\n",
    "        content=\"I would like to know the column names and types.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "for event in prebuilt_doc_agent.stream({\"messages\": followup_messages}, thread):\n",
    "    for v in event.values():\n",
    "        v[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us check the memory object to understand the persistence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in memory.list(config=None):\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create another thread instance and check if the agent can has sufficient context to answer the question. You could observe that the agent could not respond appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"20\"}}\n",
    "\n",
    "followup_messages = [\n",
    "    HumanMessage(content=\"I would like to know the column names and types.\")\n",
    "]\n",
    "\n",
    "for event in prebuilt_doc_agent.stream({\"messages\": followup_messages}, thread):\n",
    "    for v in event.values():\n",
    "        v[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real-life applications, managing memory is essential. Conversations might become pretty lengthy, and at some point, it won’t be practical to pass the whole history to LLM every time. Therefore, it’s worth trimming or filtering messages. We won’t go deep into the specifics here, but you can find guidance on it in the [LangGraph documentation](https://langchain-ai.github.io/langgraph/how-tos/memory/manage-conversation-history/). Another option to compress the conversational history is using summarization [example](https://langchain-ai.github.io/langgraph/how-tos/memory/add-summary-conversation-history/#how-to-add-summary-of-the-conversation-history)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
