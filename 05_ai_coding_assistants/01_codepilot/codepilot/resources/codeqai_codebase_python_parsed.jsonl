{"name": "env_loader", "signature": "def env_loader(env_path, required_keys=None):", "code_type": "Function", "docstring": "Args :\nenv_path = source path of .env file.\nrequired_keys = [\"OPENAI_KEY\"] #change this according to need\n\n#running/calling the function.\nconfigs = env_loader('.env', required_keys)", "line": 23, "line_from": 23, "line_to": 64, "context": {"module": "app", "file_path": "codeqai/app.py", "file_name": "app.py", "class_name": null, "function_name": null, "snippet": "def env_loader(env_path, required_keys=None):    \"\"\"    Args :    env_path = source path of .env file.    required_keys = [\"OPENAI_KEY\"] #change this according to need    #running/calling the function.    configs = env_loader('.env', required_keys)    \"\"\"    # create env file if does not exists    # parse required keys in the file if it's not None    if not os.path.exists(env_path) or os.path.getsize(env_path) == 0:        with open(env_path, \"w\") as env_f:            if required_keys:                for key in required_keys:                    env_f.write(f'{key}=\"\"\\n')            else:                pass    configs = dotenv_values(env_path)    changed = False    for key, value in configs.items():        env_key = os.getenv(key)        if not value and not env_key:            value = input(                f\"[+] Key {utils.get_bold_text(key)} is required. Please enter it's value: \"            )            configs[key] = value            changed = True        elif not value and env_key:            value = env_key            configs[key] = value            changed = True    # update the .env file if config is changed/taken from user    if changed:        with open(env_path, \"w\") as env_f:            for key, value in configs.items():                env_f.write(f'{key}=\"{value}\"\\n')    load_dotenv(env_path, override=True)"}}
{"name": "run", "signature": "def run():", "code_type": "Function", "docstring": null, "line": 67, "line_from": 67, "line_to": 235, "context": {"module": "app", "file_path": "codeqai/app.py", "file_name": "app.py", "class_name": null, "function_name": null, "snippet": "def run():    if not subprocess.run(        [\"git\", \"rev-parse\", \"--is-inside-work-tree\"], capture_output=True    ).stdout:        print(\"Not a git repository. Exiting.\")        exit()    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)    parser = argparse.ArgumentParser()    parser.add_argument(        \"action\",        choices=[\"app\", \"search\", \"chat\", \"configure\", \"sync\"],        help=\"Action to perform. 'search' will semantically search the codebase. 'chat' will chat with the codebase.\",    )    args = parser.parse_args()    if args.action == \"configure\":        create_config()        exit()    # load config    config = {}    try:        config = load_config()    except FileNotFoundError:        config = create_config()    # lookup env variables    required_keys = []    if (        config[\"llm-host\"] == LlmHost.OPENAI.value        or config[\"embeddings\"] == EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002.value    ):        required_keys.append(\"OPENAI_API_KEY\")    if (        config[\"llm-host\"] == LlmHost.AZURE_OPENAI.value        or config[\"embeddings\"] == EmbeddingsModel.AZURE_OPENAI.value    ):        required_keys.extend(            [                \"OPENAI_API_BASE\",                \"OPENAI_API_KEY\",                \"OPENAI_API_VERSION\",            ]        )    if config[\"llm-host\"] == LlmHost.ANTHROPIC.value:        required_keys.append(\"ANTHROPIC_API_KEY\")    env_path = get_config_path().replace(\"config.yaml\", \".env\")    env_loader(env_path, required_keys)    repo_name = repo.repo_name()    # init cache    create_cache_dir()    embeddings_model = Embeddings(        model=EmbeddingsModel[config[\"embeddings\"].upper().replace(\"-\", \"_\")],        deployment=(            config[\"embeddings-deployment\"]            if \"embeddings-deployment\" in config            else None        ),    )    # check if faiss.index exists    if not os.path.exists(os.path.join(get_cache_path(), f\"{repo_name}.faiss.bytes\")):        print(            f\"No vector store found for {utils.get_bold_text(repo_name)}. Initial indexing may take a few minutes.\"        )        spinner = yaspin(text=\"\ud83d\udd27 Parsing codebase...\", color=\"green\")        spinner.start()        files = repo.load_files()        documents = codeparser.parse_code_files(files)        spinner.stop()        spinner = yaspin(text=\"\ud83d\udcbe Indexing vector store...\", color=\"green\")        vector_store = VectorStore(            repo_name,            embeddings=embeddings_model.embeddings,        )        spinner.start()        vector_store.index_documents(documents)        save_vector_cache(vector_store.vector_cache, f\"{repo_name}.json\")        spinner.stop()    if args.action == \"app\":        print(\"Starting CodeQAI streamlit app...\")        run_streamlit()    else:        spinner = yaspin(text=\"\ud83d\udcbe Loading vector store...\", color=\"green\")        spinner.start()        vector_store, memory, qa = bootstrap(config, repo_name, embeddings_model)        spinner.stop()        if args.action == \"sync\":            spinner = yaspin(text=\"\ud83d\udcbe Syncing vector store...\", color=\"green\")            spinner.start()            files = repo.load_files()            vector_store.sync_documents(files)            save_vector_cache(vector_store.vector_cache, f\"{repo_name}.json\")            spinner.stop()            print(\"\u2705 Vector store synced with current git checkout.\")        console = Console()        while True:            choice = None            if args.action == \"sync\":                break            if args.action == \"search\":                search_pattern = input(\"\ud83d\udd0e Enter a search pattern: \")                spinner = yaspin(text=\"\ud83e\udd16 Processing...\", color=\"green\")                spinner.start()                similarity_result = vector_store.similarity_search(search_pattern)                spinner.stop()                for doc in similarity_result:                    language = utils.get_programming_language(                        utils.get_file_extension(doc.metadata[\"filename\"])                    )                    start_line, indentation = utils.find_starting_line_and_indent(                        doc.metadata[\"filename\"], doc.page_content                    )                    syntax = Syntax(                        indentation + doc.page_content,                        language.value,                        theme=\"monokai\",                        line_numbers=True,                        start_line=start_line,                        indent_guides=True,                    )                    print(                        doc.metadata[\"filename\"] + \" -> \" + doc.metadata[\"method_name\"]                    )                    console.print(syntax)                    print()                choice = input(\"[?] (C)ontinue search or (E)xit [C]:\").strip().lower()            elif args.action == \"chat\":                question = input(\"\ud83d\udcac Ask anything about the codebase: \")                spinner = yaspin(text=\"\ud83e\udd16 Processing...\", color=\"green\")                spinner.start()                result = qa(question)                spinner.stop()                markdown = Markdown(result[\"answer\"])                console.print(markdown)                choice = (                    input(\"[?] (C)ontinue chat, (R)eset chat or (E)xit [C]:\")                    .strip()                    .lower()                )                if choice == \"r\":                    memory.clear()                    print(\"Chat history cleared.\")            else:                print(\"Invalid action.\")                exit()            if choice == \"\" or choice == \"c\":                continue            elif choice == \"e\":                break            else:                print(\"Invalid choice. Please enter 'C', 'E', or 'R'.\")"}}
{"name": "run_streamlit", "signature": "def run_streamlit():", "code_type": "Function", "docstring": null, "line": 239, "line_from": 239, "line_to": 240, "context": {"module": "app", "file_path": "codeqai/app.py", "file_name": "app.py", "class_name": null, "function_name": null, "snippet": "def run_streamlit():    pass"}}
{"name": "main_streamlit", "signature": "def main_streamlit():", "code_type": "Function", "docstring": null, "line": 244, "line_from": 244, "line_to": 248, "context": {"module": "app", "file_path": "codeqai/app.py", "file_name": "app.py", "class_name": null, "function_name": null, "snippet": "def main_streamlit():    dirname = os.path.dirname(__file__)    filename = os.path.join(dirname, \"streamlit.py\")    args = []    stcli._main_run(filename, args)"}}
{"name": "bootstrap", "signature": "def bootstrap(config, repo_name, embeddings_model=None):", "code_type": "Function", "docstring": null, "line": 10, "line_from": 10, "line_to": 36, "context": {"module": "bootstrap", "file_path": "codeqai/bootstrap.py", "file_name": "bootstrap.py", "class_name": null, "function_name": null, "snippet": "def bootstrap(config, repo_name, embeddings_model=None):    if embeddings_model is None:        embeddings_model = Embeddings(            model=EmbeddingsModel[config[\"embeddings\"].upper().replace(\"-\", \"_\")],            deployment=(                config[\"embeddings-deployment\"]                if \"embeddings-deployment\" in config                else None            ),        )    vector_store = VectorStore(repo_name, embeddings=embeddings_model.embeddings)    vector_store.load_documents()    llm = LLM(        llm_host=LlmHost[config[\"llm-host\"].upper().replace(\"-\", \"_\")],        chat_model=config[\"chat-model\"],        deployment=config[\"model-deployment\"] if \"model-deployment\" in config else None,    )    memory = ConversationSummaryMemory(        llm=llm.chat_model, memory_key=\"chat_history\", return_messages=True    )    qa = ConversationalRetrievalChain.from_llm(        llm.chat_model, retriever=vector_store.retriever, memory=memory    )    return vector_store, memory, qa"}}
{"name": "VectorCache", "signature": "class VectorCache():", "code_type": "Class", "docstring": null, "line": 8, "line_from": 8, "line_to": 26, "context": {"module": "cache", "file_path": "codeqai/cache.py", "file_name": "cache.py", "class_name": "VectorCache", "function_name": null, "snippet": "class VectorCache:    def __init__(self, filename, vector_ids, commit_hash):        self.filename = filename        self.vector_ids = vector_ids        self.commit_hash = commit_hash    @classmethod    def from_json(cls, json_data) -> \"VectorCache\":        filename = json_data.get(\"filename\")        vector_ids = json_data.get(\"vector_ids\", [])        commit_hash = json_data.get(\"commit_hash\", \"\")        return cls(filename, vector_ids, commit_hash)    def to_json(self):        return {            \"filename\": self.filename,            \"commit_hash\": self.commit_hash,            \"vector_ids\": self.vector_ids,        }"}}
{"name": "__init__", "signature": "def __init__(self, filename, vector_ids, commit_hash):", "code_type": "Function", "docstring": null, "line": 9, "line_from": 9, "line_to": 12, "context": {"module": "cache", "file_path": "codeqai/cache.py", "file_name": "cache.py", "class_name": "VectorCache", "function_name": null, "snippet": "def __init__(self, filename, vector_ids, commit_hash):        self.filename = filename        self.vector_ids = vector_ids        self.commit_hash = commit_hash"}}
{"name": "from_json", "signature": "def from_json(cls, json_data):", "code_type": "Function", "docstring": null, "line": 15, "line_from": 15, "line_to": 19, "context": {"module": "cache", "file_path": "codeqai/cache.py", "file_name": "cache.py", "class_name": "VectorCache", "function_name": null, "snippet": "def from_json(cls, json_data) -> \"VectorCache\":        filename = json_data.get(\"filename\")        vector_ids = json_data.get(\"vector_ids\", [])        commit_hash = json_data.get(\"commit_hash\", \"\")        return cls(filename, vector_ids, commit_hash)"}}
{"name": "to_json", "signature": "def to_json(self):", "code_type": "Function", "docstring": null, "line": 21, "line_from": 21, "line_to": 26, "context": {"module": "cache", "file_path": "codeqai/cache.py", "file_name": "cache.py", "class_name": "VectorCache", "function_name": null, "snippet": "def to_json(self):        return {            \"filename\": self.filename,            \"commit_hash\": self.commit_hash,            \"vector_ids\": self.vector_ids,        }"}}
{"name": "load_vector_cache", "signature": "def load_vector_cache(filename):", "code_type": "Function", "docstring": null, "line": 29, "line_from": 29, "line_to": 37, "context": {"module": "cache", "file_path": "codeqai/cache.py", "file_name": "cache.py", "class_name": null, "function_name": null, "snippet": "def load_vector_cache(filename) -> Dict[str, VectorCache]:    with open(        get_cache_path() + \"/\" + filename, \"r\", encoding=\"utf-8\"    ) as vector_cache_file:        vector_cache_json = json.load(vector_cache_file)    vector_cache = {}    for key, value in vector_cache_json.items():        vector_cache[key] = VectorCache.from_json(value)    return vector_cache"}}
{"name": "save_vector_cache", "signature": "def save_vector_cache(vector_cache, filename):", "code_type": "Function", "docstring": null, "line": 40, "line_from": 40, "line_to": 44, "context": {"module": "cache", "file_path": "codeqai/cache.py", "file_name": "cache.py", "class_name": null, "function_name": null, "snippet": "def save_vector_cache(vector_cache, filename):    with open(        get_cache_path() + \"/\" + filename, \"w\", encoding=\"utf-8\"    ) as vector_cache_file:        json.dump(vector_cache, default=VectorCache.to_json, fp=vector_cache_file)"}}
{"name": "get_cache_path", "signature": "def get_cache_path():", "code_type": "Function", "docstring": null, "line": 47, "line_from": 47, "line_to": 59, "context": {"module": "cache", "file_path": "codeqai/cache.py", "file_name": "cache.py", "class_name": null, "function_name": null, "snippet": "def get_cache_path():    system = platform.system()    if system == \"Linux\" or system == \"Darwin\":        user_home = os.path.expanduser(\"~\")        cache_dir = os.path.join(user_home, \".cache\", \"codeqai\")    elif system == \"Windows\":        user_home = os.path.expanduser(\"~\")        cache_dir = os.path.join(user_home, \"AppData\", \"Local\", \"codeqai\")    else:        raise NotImplementedError(f\"Unsupported platform: {system}\")    return cache_dir"}}
{"name": "create_cache_dir", "signature": "def create_cache_dir():", "code_type": "Function", "docstring": null, "line": 62, "line_from": 62, "line_to": 65, "context": {"module": "cache", "file_path": "codeqai/cache.py", "file_name": "cache.py", "class_name": null, "function_name": null, "snippet": "def create_cache_dir():    if not os.path.exists(get_cache_path()):        path = Path(get_cache_path())        path.mkdir(parents=True, exist_ok=True)"}}
{"name": "parse_code_files", "signature": "def parse_code_files(code_files):", "code_type": "Function", "docstring": null, "line": 11, "line_from": 11, "line_to": 59, "context": {"module": "codeparser", "file_path": "codeqai/codeparser.py", "file_name": "codeparser.py", "class_name": null, "function_name": null, "snippet": "def parse_code_files(code_files: list[str]) -> list[Document]:    documents = []    code_splitter = None    for code_file in code_files:        with open(code_file, \"r\", encoding=\"utf-8\") as file:            file_bytes = file.read().encode()            commit_hash = repo.get_commit_hash(code_file)            file_extension = utils.get_file_extension(code_file)            programming_language = utils.get_programming_language(file_extension)            if programming_language == Language.UNKNOWN:                continue            langchain_language = utils.get_langchain_language(programming_language)            if langchain_language:                code_splitter = RecursiveCharacterTextSplitter.from_language(                    language=langchain_language,                    chunk_size=512,                    chunk_overlap=128,                )            treesitter_parser = Treesitter.create_treesitter(programming_language)            treesitterNodes: list[TreesitterMethodNode] = treesitter_parser.parse(                file_bytes            )            for node in treesitterNodes:                method_source_code = node.method_source_code                filename = os.path.basename(code_file)                if node.doc_comment and programming_language != Language.PYTHON:                    method_source_code = node.doc_comment + \"\\n\" + method_source_code                splitted_documents = [method_source_code]                if code_splitter:                    splitted_documents = code_splitter.split_text(method_source_code)                for splitted_document in splitted_documents:                    document = Document(                        page_content=splitted_document,                        metadata={                            \"filename\": filename,                            \"method_name\": node.name,                            \"commit_hash\": commit_hash,                        },                    )                    documents.append(document)    return documents"}}
{"name": "get_config_path", "signature": "def get_config_path():", "code_type": "Function", "docstring": null, "line": 10, "line_from": 10, "line_to": 24, "context": {"module": "config", "file_path": "codeqai/config.py", "file_name": "config.py", "class_name": null, "function_name": null, "snippet": "def get_config_path():    system = platform.system()    if system == \"Linux\" or system == \"Darwin\":        user_home = os.path.expanduser(\"~\")        config_dir = os.path.join(user_home, \".config\", \"codeqai\")    elif system == \"Windows\":        user_home = os.path.expanduser(\"~\")        config_dir = os.path.join(user_home, \"AppData\", \"Roaming\", \"codeqai\")    else:        raise NotImplementedError(f\"Unsupported platform: {system}\")    config_file_path = os.path.join(config_dir, \"config.yaml\")    return config_file_path"}}
{"name": "load_config", "signature": "def load_config():", "code_type": "Function", "docstring": null, "line": 27, "line_from": 27, "line_to": 30, "context": {"module": "config", "file_path": "codeqai/config.py", "file_name": "config.py", "class_name": null, "function_name": null, "snippet": "def load_config():    with open(get_config_path(), \"r\", encoding=\"utf-8\") as config_file:        config = yaml.safe_load(config_file)    return config"}}
{"name": "save_config", "signature": "def save_config(config):", "code_type": "Function", "docstring": null, "line": 33, "line_from": 33, "line_to": 35, "context": {"module": "config", "file_path": "codeqai/config.py", "file_name": "config.py", "class_name": null, "function_name": null, "snippet": "def save_config(config):    with open(get_config_path(), \"w\", encoding=\"utf-8\") as config_file:        yaml.dump(config, config_file, default_flow_style=False)"}}
{"name": "create_config", "signature": "def create_config():", "code_type": "Function", "docstring": null, "line": 38, "line_from": 38, "line_to": 210, "context": {"module": "config", "file_path": "codeqai/config.py", "file_name": "config.py", "class_name": null, "function_name": null, "snippet": "def create_config():    os.makedirs(os.path.dirname(get_config_path()), exist_ok=True)    questions = [        inquirer.Confirm(            \"confirm\",            message=\"Do you want to use local embedding models?\",            default=False,        ),    ]    confirm = inquirer.prompt(questions)    if confirm and confirm[\"confirm\"]:        questions = [            inquirer.List(                \"embeddings\",                message=\"Which local embeddings model do you want to use?\",                choices=[                    EmbeddingsModel.INSTRUCTOR_LARGE.value,                    EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MPNET_BASE_V2.value,                    EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MINILM_L6_V2.value,                ],                default=EmbeddingsModel.INSTRUCTOR_LARGE.value,            ),        ]    else:        questions = [            inquirer.List(                \"embeddings\",                message=\"Which remote embeddings do you want to use?\",                choices=[                    EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002.value,                    EmbeddingsModel.AZURE_OPENAI.value,                ],                default=EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002.value,            ),        ]    answersEmbedding = inquirer.prompt(questions)    questions = [        inquirer.Confirm(            \"confirm\", message=\"Do you want to use local chat models?\", default=False        ),    ]    confirm = inquirer.prompt(questions)    if confirm and confirm[\"confirm\"]:        questions = [            inquirer.List(                \"llm-host\",                message=\"Which local LLM host do you want to use?\",                choices=[                    LlmHost.LLAMACPP.value,                    LlmHost.OLLAMA.value,                ],                default=LlmHost.LLAMACPP.value,            ),        ]    else:        questions = [            inquirer.List(                \"llm-host\",                message=\"Which remote LLM do you want to use?\",                choices=[                    LlmHost.OPENAI.value,                    LlmHost.AZURE_OPENAI.value,                    LlmHost.ANTHROPIC.value,                ],                default=LlmHost.OPENAI.value,            ),        ]    answersLlm = inquirer.prompt(questions)    if confirm and answersEmbedding and answersLlm:        config = {            \"embeddings\": answersEmbedding[\"embeddings\"],            \"llm-host\": answersLlm[\"llm-host\"],        }        if config[\"embeddings\"] == EmbeddingsModel.AZURE_OPENAI.value:            questions = [                inquirer.Text(                    \"deployment\",                    message=\"Please enter the Azure OpenAI embeddings deployment name.\",                    default=\"\",                ),            ]            deployment_answer = inquirer.prompt(questions)            if deployment_answer and deployment_answer[\"deployment\"]:                config[\"embeddings-deployment\"] = deployment_answer[\"deployment\"]        if config[\"llm-host\"] == LlmHost.AZURE_OPENAI.value:            questions = [                inquirer.Text(                    \"deployment\",                    message=\"Please enter the Azure OpenAI model deployment name\",                    default=\"\",                ),            ]            deployment_answer = inquirer.prompt(questions)            if deployment_answer and deployment_answer[\"deployment\"]:                config[\"model-deployment\"] = deployment_answer[\"deployment\"]                config[\"chat-model\"] = deployment_answer[\"deployment\"]        elif config[\"llm-host\"] == LlmHost.LLAMACPP.value:            questions = [                inquirer.Text(                    \"chat-model\",                    message=\"Please enter the path to the LLM model\",                    default=\"\",                ),            ]        elif config[\"llm-host\"] == LlmHost.OLLAMA.value:            questions = [                inquirer.List(                    \"chat-model\",                    message=\"Which Ollama chat model do you want to use?\",                    choices=[                        \"llama2\",                        \"llama2:13b\",                        \"llama2:70b\",                        \"codellama\",                    ],                    default=\"llama2:13b\",                ),            ]        elif config[\"llm-host\"] == LlmHost.OPENAI.value:            questions = [                inquirer.List(                    \"chat-model\",                    message=\"Which OpenAI chat model do you want to use?\",                    choices=[                        \"gpt-3.5-turbo\",                        \"gpt-3.5-turbo-16k\",                        \"gpt-4\",                        \"gpt-4-turbo\",                        \"gpt-4o\",                    ],                    default=\"gpt-3.5-turbo\",                ),            ]        elif config[\"llm-host\"] == LlmHost.ANTHROPIC.value:            questions = [                inquirer.List(                    \"chat-model\",                    message=\"Which Anthropic chat model do you want to use?\",                    choices=[                        \"claude-3-opus-20240229\",                        \"claude-3-sonnet-20240229\",                        \"claude-3-haiku-20240307\",                    ],                    default=\"claude-3-opus-20240229\",                ),            ]        # Check if \"chat-model\" is already present in the case of Azure_OpenAI        if \"chat-model\" not in config:            answersChatmodel = inquirer.prompt(questions)            if answersChatmodel and answersChatmodel[\"chat-model\"]:                config[\"chat-model\"] = answersChatmodel[\"chat-model\"]        save_config(config)        return config    return {}"}}
{"name": "Language", "signature": "class Language(Enum):", "code_type": "Class", "docstring": null, "line": 4, "line_from": 4, "line_to": 20, "context": {"module": "constants", "file_path": "codeqai/constants.py", "file_name": "constants.py", "class_name": "Language", "function_name": null, "snippet": "class Language(Enum):    PYTHON = \"python\"    JAVASCRIPT = \"javascript\"    TYPESCRIPT = \"typescript\"    JAVA = \"java\"    CPP = \"cpp\"    C = \"c\"    GO = \"go\"    RUST = \"rust\"    KOTLIN = \"kotlin\"    C_SHARP = \"c_sharp\"    OBJECTIVE_C = \"objective_c\"    SCALA = \"scala\"    LUA = \"lua\"    HASKELL = \"haskell\"    RUBY = \"ruby\"    UNKNOWN = \"unknown\""}}
{"name": "EmbeddingsModel", "signature": "class EmbeddingsModel(Enum):", "code_type": "Class", "docstring": null, "line": 23, "line_from": 23, "line_to": 28, "context": {"module": "constants", "file_path": "codeqai/constants.py", "file_name": "constants.py", "class_name": "EmbeddingsModel", "function_name": null, "snippet": "class EmbeddingsModel(Enum):    SENTENCETRANSFORMERS_ALL_MPNET_BASE_V2 = \"SentenceTransformers-all-mpnet-base-v2\"    SENTENCETRANSFORMERS_ALL_MINILM_L6_V2 = \"SentenceTransformers-all-MiniLM-L6-v2\"    INSTRUCTOR_LARGE = \"Instructor-Large\"    OPENAI_TEXT_EMBEDDING_ADA_002 = \"OpenAI-text-embedding-ada-002\"    AZURE_OPENAI = \"Azure-OpenAI\""}}
{"name": "LlmHost", "signature": "class LlmHost(Enum):", "code_type": "Class", "docstring": null, "line": 31, "line_from": 31, "line_to": 36, "context": {"module": "constants", "file_path": "codeqai/constants.py", "file_name": "constants.py", "class_name": "LlmHost", "function_name": null, "snippet": "class LlmHost(Enum):    LLAMACPP = \"Llamacpp\"    OLLAMA = \"Ollama\"    OPENAI = \"OpenAI\"    AZURE_OPENAI = \"Azure-OpenAI\"    ANTHROPIC = \"Anthropic\""}}
{"name": "Embeddings", "signature": "class Embeddings():", "code_type": "Class", "docstring": null, "line": 9, "line_from": 9, "line_to": 103, "context": {"module": "embeddings", "file_path": "codeqai/embeddings.py", "file_name": "embeddings.py", "class_name": "Embeddings", "function_name": null, "snippet": "class Embeddings:    def __init__(        self,        model=EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002,        deployment=None,    ):        if model == EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002:            self.embeddings = OpenAIEmbeddings(                client=None, model=\"text-embedding-ada-002\"            )        elif model == EmbeddingsModel.AZURE_OPENAI and deployment:            self.embeddings = AzureOpenAIEmbeddings(client=None, deployment=deployment)        else:            try:                import sentence_transformers  # noqa: F401            except ImportError:                self._install_sentence_transformers()            if model == EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MPNET_BASE_V2:                self.embeddings = HuggingFaceEmbeddings()            elif model == EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MINILM_L6_V2:                self.embeddings = HuggingFaceEmbeddings(                    model_name=EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MINILM_L6_V2.value.replace(                        \"SentenceTransformers-\", \"\"                    )                )            elif model == EmbeddingsModel.INSTRUCTOR_LARGE:                try:                    from InstructorEmbedding import INSTRUCTOR  # noqa: F401                except ImportError:                    self._install_instructor_embedding()                self.embeddings = HuggingFaceEmbeddings(                    model_name=\"hkunlp/instructor-xl\"                )    def _install_sentence_transformers(self):        question = [            inquirer.Confirm(                \"confirm\",                message=f\"{utils.get_bold_text('SentenceTransformers')} not found in this python environment. Do you want to install it now?\",                default=True,            ),        ]        answers = inquirer.prompt(question)        if answers and answers[\"confirm\"]:            import subprocess            import sys            try:                subprocess.run(                    [                        sys.executable,                        \"-m\",                        \"pip\",                        \"install\",                        \"sentence_transformers\",                    ],                    check=True,                )            except subprocess.CalledProcessError as e:                print(f\"Error during sentence_transformers installation: {e}\")        else:            exit(\"sentence_transformers is required for local embeddings.\")    def _install_instructor_embedding(self):        question = [            inquirer.Confirm(                \"confirm\",                message=f\"{utils.get_bold_text('InstructorEmbedding')} not found in this python environment. Do you want to install it now?\",                default=True,            ),        ]        answers = inquirer.prompt(question)        if answers and answers[\"confirm\"]:            import subprocess            import sys            try:                subprocess.run(                    [                        sys.executable,                        \"-m\",                        \"pip\",                        \"install\",                        \"InstructorEmbedding\",                    ],                    check=True,                )            except subprocess.CalledProcessError as e:                print(f\"Error during sentence_transformers installation: {e}\")        else:            exit(\"InstructorEmbedding is required for local embeddings.\")"}}
{"name": "__init__", "signature": "def __init__(self, model=EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002, deployment=None):", "code_type": "Function", "docstring": null, "line": 10, "line_from": 10, "line_to": 43, "context": {"module": "embeddings", "file_path": "codeqai/embeddings.py", "file_name": "embeddings.py", "class_name": "Embeddings", "function_name": null, "snippet": "def __init__(        self,        model=EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002,        deployment=None,    ):        if model == EmbeddingsModel.OPENAI_TEXT_EMBEDDING_ADA_002:            self.embeddings = OpenAIEmbeddings(                client=None, model=\"text-embedding-ada-002\"            )        elif model == EmbeddingsModel.AZURE_OPENAI and deployment:            self.embeddings = AzureOpenAIEmbeddings(client=None, deployment=deployment)        else:            try:                import sentence_transformers  # noqa: F401            except ImportError:                self._install_sentence_transformers()            if model == EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MPNET_BASE_V2:                self.embeddings = HuggingFaceEmbeddings()            elif model == EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MINILM_L6_V2:                self.embeddings = HuggingFaceEmbeddings(                    model_name=EmbeddingsModel.SENTENCETRANSFORMERS_ALL_MINILM_L6_V2.value.replace(                        \"SentenceTransformers-\", \"\"                    )                )            elif model == EmbeddingsModel.INSTRUCTOR_LARGE:                try:                    from InstructorEmbedding import INSTRUCTOR  # noqa: F401                except ImportError:                    self._install_instructor_embedding()                self.embeddings = HuggingFaceEmbeddings(                    model_name=\"hkunlp/instructor-xl\"                )"}}
{"name": "_install_sentence_transformers", "signature": "def _install_sentence_transformers(self):", "code_type": "Function", "docstring": null, "line": 45, "line_from": 45, "line_to": 73, "context": {"module": "embeddings", "file_path": "codeqai/embeddings.py", "file_name": "embeddings.py", "class_name": "Embeddings", "function_name": null, "snippet": "def _install_sentence_transformers(self):        question = [            inquirer.Confirm(                \"confirm\",                message=f\"{utils.get_bold_text('SentenceTransformers')} not found in this python environment. Do you want to install it now?\",                default=True,            ),        ]        answers = inquirer.prompt(question)        if answers and answers[\"confirm\"]:            import subprocess            import sys            try:                subprocess.run(                    [                        sys.executable,                        \"-m\",                        \"pip\",                        \"install\",                        \"sentence_transformers\",                    ],                    check=True,                )            except subprocess.CalledProcessError as e:                print(f\"Error during sentence_transformers installation: {e}\")        else:            exit(\"sentence_transformers is required for local embeddings.\")"}}
{"name": "_install_instructor_embedding", "signature": "def _install_instructor_embedding(self):", "code_type": "Function", "docstring": null, "line": 75, "line_from": 75, "line_to": 103, "context": {"module": "embeddings", "file_path": "codeqai/embeddings.py", "file_name": "embeddings.py", "class_name": "Embeddings", "function_name": null, "snippet": "def _install_instructor_embedding(self):        question = [            inquirer.Confirm(                \"confirm\",                message=f\"{utils.get_bold_text('InstructorEmbedding')} not found in this python environment. Do you want to install it now?\",                default=True,            ),        ]        answers = inquirer.prompt(question)        if answers and answers[\"confirm\"]:            import subprocess            import sys            try:                subprocess.run(                    [                        sys.executable,                        \"-m\",                        \"pip\",                        \"install\",                        \"InstructorEmbedding\",                    ],                    check=True,                )            except subprocess.CalledProcessError as e:                print(f\"Error during sentence_transformers installation: {e}\")        else:            exit(\"InstructorEmbedding is required for local embeddings.\")"}}
{"name": "LLM", "signature": "class LLM():", "code_type": "Class", "docstring": null, "line": 17, "line_from": 17, "line_to": 152, "context": {"module": "llm", "file_path": "codeqai/llm.py", "file_name": "llm.py", "class_name": "LLM", "function_name": null, "snippet": "class LLM:    def __init__(self, llm_host: LlmHost, chat_model: str, deployment=None):        if llm_host == LlmHost.OPENAI:            self.chat_model = ChatOpenAI(                temperature=0.9, max_tokens=2048, model=chat_model            )        elif llm_host == LlmHost.AZURE_OPENAI and deployment:            azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")            if azure_openai_endpoint:                self.chat_model = AzureChatOpenAI(                    azure_endpoint=azure_openai_endpoint,                    temperature=0.9,                    max_tokens=2048,                    deployment_name=deployment,                    model=chat_model,                )            else:                raise ValueError(                    \"Azure OpenAI requires environment variable AZURE_OPENAI_ENDPOINT to be set.\"                )        elif llm_host == LlmHost.ANTHROPIC:            self.chat_model = ChatAnthropic(                temperature=0.9, max_tokens=2048, model_name=chat_model            )        elif llm_host == LlmHost.LLAMACPP:            self.install_llama_cpp()            self.chat_model = LlamaCpp(                model_path=chat_model,                temperature=0.9,                max_tokens=2048,                verbose=False,            )        elif llm_host == LlmHost.OLLAMA:            self.chat_model = Ollama(                base_url=\"http://localhost:11434\",                model=chat_model,                callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),            )    def install_llama_cpp(self):        try:            from llama_cpp import Llama  # noqa: F401        except ImportError:            question = [                inquirer.Confirm(                    \"confirm\",                    message=f\"Local LLM interface package not found. Install {utils.get_bold_text('llama-cpp-python')}?\",                    default=True,                ),            ]            answers = inquirer.prompt(question)            if answers and answers[\"confirm\"]:                import platform                def check_command(command):                    try:                        subprocess.run(                            command,                            check=True,                            stdout=subprocess.PIPE,                            stderr=subprocess.PIPE,                        )                        return True                    except subprocess.CalledProcessError:                        return False                    except FileNotFoundError:                        return False                def install_llama(backend):                    env_vars = {\"FORCE_CMAKE\": \"1\"}                    if backend == \"cuBLAS\":                        env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=on\"                    elif backend == \"hipBLAS\":                        env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_HIPBLAS=on\"                    elif backend == \"Metal\":                        env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_METAL=on\"                    else:  # Default to OpenBLAS                        env_vars[\"CMAKE_ARGS\"] = (                            \"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\"                        )                    try:                        subprocess.run(                            [                                sys.executable,                                \"-m\",                                \"pip\",                                \"install\",                                \"llama-cpp-python\",                            ],                            env={**os.environ, **env_vars},                            check=True,                        )                    except subprocess.CalledProcessError as e:                        print(f\"Error during installation with {backend}: {e}\")                def supports_metal():                    # Check for macOS version                    if platform.system() == \"Darwin\":                        mac_version = tuple(map(int, platform.mac_ver()[0].split(\".\")))                        # Metal requires macOS 10.11 or later                        if mac_version >= (10, 11):                            return True                    return False                # Check system capabilities                if check_command([\"nvidia-smi\"]):                    install_llama(\"cuBLAS\")                elif check_command([\"rocminfo\"]):                    install_llama(\"hipBLAS\")                elif supports_metal():                    install_llama(\"Metal\")                else:                    install_llama(\"OpenBLAS\")                print(\"Finished downloading `Code-Llama` interface.\")                # Check if on macOS                if platform.system() == \"Darwin\":                    # Check if it's Apple Silicon                    if platform.machine() != \"arm64\":                        print(                            \"Warning: You are using Apple Silicon (M1/M2) Mac but your Python is not of 'arm64' architecture.\"                        )                        print(                            \"The llama.ccp x86 version will be 10x slower on Apple Silicon (M1/M2) Mac.\"                        )                        print(                            \"\\nTo install the correct version of Python that supports 'arm64' architecture visit:\"                            \"https://github.com/conda-forge/miniforge\"                        )            else:                exit(\"llama-cpp-python is required for local LLM.\")"}}
{"name": "__init__", "signature": "def __init__(self, llm_host, chat_model, deployment=None):", "code_type": "Function", "docstring": null, "line": 18, "line_from": 18, "line_to": 54, "context": {"module": "llm", "file_path": "codeqai/llm.py", "file_name": "llm.py", "class_name": "LLM", "function_name": null, "snippet": "def __init__(self, llm_host: LlmHost, chat_model: str, deployment=None):        if llm_host == LlmHost.OPENAI:            self.chat_model = ChatOpenAI(                temperature=0.9, max_tokens=2048, model=chat_model            )        elif llm_host == LlmHost.AZURE_OPENAI and deployment:            azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")            if azure_openai_endpoint:                self.chat_model = AzureChatOpenAI(                    azure_endpoint=azure_openai_endpoint,                    temperature=0.9,                    max_tokens=2048,                    deployment_name=deployment,                    model=chat_model,                )            else:                raise ValueError(                    \"Azure OpenAI requires environment variable AZURE_OPENAI_ENDPOINT to be set.\"                )        elif llm_host == LlmHost.ANTHROPIC:            self.chat_model = ChatAnthropic(                temperature=0.9, max_tokens=2048, model_name=chat_model            )        elif llm_host == LlmHost.LLAMACPP:            self.install_llama_cpp()            self.chat_model = LlamaCpp(                model_path=chat_model,                temperature=0.9,                max_tokens=2048,                verbose=False,            )        elif llm_host == LlmHost.OLLAMA:            self.chat_model = Ollama(                base_url=\"http://localhost:11434\",                model=chat_model,                callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),            )"}}
{"name": "install_llama_cpp", "signature": "def install_llama_cpp(self):", "code_type": "Function", "docstring": null, "line": 56, "line_from": 56, "line_to": 152, "context": {"module": "llm", "file_path": "codeqai/llm.py", "file_name": "llm.py", "class_name": "LLM", "function_name": null, "snippet": "def install_llama_cpp(self):        try:            from llama_cpp import Llama  # noqa: F401        except ImportError:            question = [                inquirer.Confirm(                    \"confirm\",                    message=f\"Local LLM interface package not found. Install {utils.get_bold_text('llama-cpp-python')}?\",                    default=True,                ),            ]            answers = inquirer.prompt(question)            if answers and answers[\"confirm\"]:                import platform                def check_command(command):                    try:                        subprocess.run(                            command,                            check=True,                            stdout=subprocess.PIPE,                            stderr=subprocess.PIPE,                        )                        return True                    except subprocess.CalledProcessError:                        return False                    except FileNotFoundError:                        return False                def install_llama(backend):                    env_vars = {\"FORCE_CMAKE\": \"1\"}                    if backend == \"cuBLAS\":                        env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=on\"                    elif backend == \"hipBLAS\":                        env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_HIPBLAS=on\"                    elif backend == \"Metal\":                        env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_METAL=on\"                    else:  # Default to OpenBLAS                        env_vars[\"CMAKE_ARGS\"] = (                            \"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\"                        )                    try:                        subprocess.run(                            [                                sys.executable,                                \"-m\",                                \"pip\",                                \"install\",                                \"llama-cpp-python\",                            ],                            env={**os.environ, **env_vars},                            check=True,                        )                    except subprocess.CalledProcessError as e:                        print(f\"Error during installation with {backend}: {e}\")                def supports_metal():                    # Check for macOS version                    if platform.system() == \"Darwin\":                        mac_version = tuple(map(int, platform.mac_ver()[0].split(\".\")))                        # Metal requires macOS 10.11 or later                        if mac_version >= (10, 11):                            return True                    return False                # Check system capabilities                if check_command([\"nvidia-smi\"]):                    install_llama(\"cuBLAS\")                elif check_command([\"rocminfo\"]):                    install_llama(\"hipBLAS\")                elif supports_metal():                    install_llama(\"Metal\")                else:                    install_llama(\"OpenBLAS\")                print(\"Finished downloading `Code-Llama` interface.\")                # Check if on macOS                if platform.system() == \"Darwin\":                    # Check if it's Apple Silicon                    if platform.machine() != \"arm64\":                        print(                            \"Warning: You are using Apple Silicon (M1/M2) Mac but your Python is not of 'arm64' architecture.\"                        )                        print(                            \"The llama.ccp x86 version will be 10x slower on Apple Silicon (M1/M2) Mac.\"                        )                        print(                            \"\\nTo install the correct version of Python that supports 'arm64' architecture visit:\"                            \"https://github.com/conda-forge/miniforge\"                        )            else:                exit(\"llama-cpp-python is required for local LLM.\")"}}
{"name": "check_command", "signature": "def check_command(command):", "code_type": "Function", "docstring": null, "line": 72, "line_from": 72, "line_to": 84, "context": {"module": "llm", "file_path": "codeqai/llm.py", "file_name": "llm.py", "class_name": "LLM", "function_name": null, "snippet": "def check_command(command):                    try:                        subprocess.run(                            command,                            check=True,                            stdout=subprocess.PIPE,                            stderr=subprocess.PIPE,                        )                        return True                    except subprocess.CalledProcessError:                        return False                    except FileNotFoundError:                        return False"}}
{"name": "install_llama", "signature": "def install_llama(backend):", "code_type": "Function", "docstring": null, "line": 86, "line_from": 86, "line_to": 113, "context": {"module": "llm", "file_path": "codeqai/llm.py", "file_name": "llm.py", "class_name": "LLM", "function_name": null, "snippet": "def install_llama(backend):                    env_vars = {\"FORCE_CMAKE\": \"1\"}                    if backend == \"cuBLAS\":                        env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_CUBLAS=on\"                    elif backend == \"hipBLAS\":                        env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_HIPBLAS=on\"                    elif backend == \"Metal\":                        env_vars[\"CMAKE_ARGS\"] = \"-DLLAMA_METAL=on\"                    else:  # Default to OpenBLAS                        env_vars[\"CMAKE_ARGS\"] = (                            \"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\"                        )                    try:                        subprocess.run(                            [                                sys.executable,                                \"-m\",                                \"pip\",                                \"install\",                                \"llama-cpp-python\",                            ],                            env={**os.environ, **env_vars},                            check=True,                        )                    except subprocess.CalledProcessError as e:                        print(f\"Error during installation with {backend}: {e}\")"}}
{"name": "supports_metal", "signature": "def supports_metal():", "code_type": "Function", "docstring": null, "line": 115, "line_from": 115, "line_to": 122, "context": {"module": "llm", "file_path": "codeqai/llm.py", "file_name": "llm.py", "class_name": "LLM", "function_name": null, "snippet": "def supports_metal():                    # Check for macOS version                    if platform.system() == \"Darwin\":                        mac_version = tuple(map(int, platform.mac_ver()[0].split(\".\")))                        # Metal requires macOS 10.11 or later                        if mac_version >= (10, 11):                            return True                    return False"}}
{"name": "load_gitignore", "signature": "def load_gitignore(root_dir):", "code_type": "Function", "docstring": null, "line": 8, "line_from": 8, "line_to": 17, "context": {"module": "parse_python", "file_path": "codeqai/parse_python.py", "file_name": "parse_python.py", "class_name": null, "function_name": null, "snippet": "def load_gitignore(root_dir):    gitignore_path = os.path.join(root_dir, '.gitignore')    if os.path.exists(gitignore_path):        with open(gitignore_path, 'r') as gitignore_file:            spec = PathSpec.from_lines(GitWildMatchPattern, gitignore_file)        print(f\"Loaded .gitignore from {gitignore_path}\")    else:        spec = PathSpec(GitWildMatchPattern, [])        print(\"No .gitignore file found. Proceeding without ignore patterns.\")    return spec"}}
{"name": "parse_python_file", "signature": "def parse_python_file(file_path):", "code_type": "Function", "docstring": null, "line": 19, "line_from": 19, "line_to": 30, "context": {"module": "parse_python", "file_path": "codeqai/parse_python.py", "file_name": "parse_python.py", "class_name": null, "function_name": null, "snippet": "def parse_python_file(file_path):    with open(file_path, 'r') as file:        content = file.read()        tree = ast.parse(content)        items = []    for node in ast.walk(tree):        if isinstance(node, (ast.FunctionDef, ast.ClassDef)):            items.append(extract_info(node, content, file_path))        print(f\"  Extracted {len(items)} items from {file_path}\")    return items"}}
{"name": "extract_info", "signature": "def extract_info(node, content, file_path):", "code_type": "Function", "docstring": null, "line": 32, "line_from": 32, "line_to": 50, "context": {"module": "parse_python", "file_path": "codeqai/parse_python.py", "file_name": "parse_python.py", "class_name": null, "function_name": null, "snippet": "def extract_info(node, content, file_path):    lines = content.split('\\n')        return {        \"name\": node.name,        \"signature\": ast.get_source_segment(content, node),        \"code_type\": type(node).__name__,        \"docstring\": ast.get_docstring(node),        \"line\": node.lineno,        \"line_from\": node.lineno,        \"line_to\": node.end_lineno,        \"context\": {            \"module\": os.path.splitext(os.path.basename(file_path))[0],            \"file_path\": file_path,            \"file_name\": os.path.basename(file_path),            \"class_name\": get_parent_class(node),            \"snippet\": '\\n'.join(lines[node.lineno-1:node.end_lineno])        }    }"}}
{"name": "get_parent_class", "signature": "def get_parent_class(node):", "code_type": "Function", "docstring": null, "line": 52, "line_from": 52, "line_to": 54, "context": {"module": "parse_python", "file_path": "codeqai/parse_python.py", "file_name": "parse_python.py", "class_name": null, "function_name": null, "snippet": "def get_parent_class(node):    parent = getattr(node, 'parent', None)    return parent.name if isinstance(parent, ast.ClassDef) else None"}}
{"name": "process_directory", "signature": "def process_directory(directory):", "code_type": "Function", "docstring": null, "line": 56, "line_from": 56, "line_to": 91, "context": {"module": "parse_python", "file_path": "codeqai/parse_python.py", "file_name": "parse_python.py", "class_name": null, "function_name": null, "snippet": "def process_directory(directory):    gitignore_spec = load_gitignore(directory)        total_files = 0    processed_files = 0    total_items = 0        with jsonlines.open('output.jsonl', mode='w') as writer:        for root, dirs, files in os.walk(directory):            # Remove ignored directories            dirs[:] = [d for d in dirs if not gitignore_spec.match_file(os.path.join(root, d))]                        python_files = [f for f in files if f.endswith('.py')]            total_files += len(python_files)                        for file in python_files:                file_path = os.path.join(root, file)                relative_path = os.path.relpath(file_path, directory)                                # Skip ignored files                if gitignore_spec.match_file(relative_path):                    print(f\"Skipping ignored file: {relative_path}\")                    continue                                print(f\"Processing file {processed_files + 1}/{total_files}: {relative_path}\")                items = parse_python_file(file_path)                for item in items:                    writer.write(item)                                processed_files += 1                total_items += len(items)        print(f\"\\nProcessing complete!\")    print(f\"Processed {processed_files} Python files out of {total_files} total files.\")    print(f\"Extracted {total_items} items in total.\")    print(f\"Results written to output.jsonl\")"}}
{"name": "repo_name", "signature": "def repo_name():", "code_type": "Function", "docstring": null, "line": 7, "line_from": 7, "line_to": 8, "context": {"module": "repo", "file_path": "codeqai/repo.py", "file_name": "repo.py", "class_name": null, "function_name": null, "snippet": "def repo_name():    return get_git_root(os.getcwd()).split(\"/\")[-1]"}}
{"name": "get_git_root", "signature": "def get_git_root(path):", "code_type": "Function", "docstring": null, "line": 11, "line_from": 11, "line_to": 14, "context": {"module": "repo", "file_path": "codeqai/repo.py", "file_name": "repo.py", "class_name": null, "function_name": null, "snippet": "def get_git_root(path):    git_repo = Repo(path, search_parent_directories=True)    git_root = git_repo.git.rev_parse(\"--show-toplevel\")    return git_root"}}
{"name": "find_file_in_git_repo", "signature": "def find_file_in_git_repo(file_name):", "code_type": "Function", "docstring": null, "line": 17, "line_from": 17, "line_to": 25, "context": {"module": "repo", "file_path": "codeqai/repo.py", "file_name": "repo.py", "class_name": null, "function_name": null, "snippet": "def find_file_in_git_repo(file_name):    git_root = get_git_root(os.getcwd())    for root, dirs, files in os.walk(git_root):        if any(blacklist in root for blacklist in BLACKLIST_DIR):            continue        for file in files:            if file == file_name:                return os.path.join(root, file)"}}
{"name": "load_files", "signature": "def load_files():", "code_type": "Function", "docstring": null, "line": 28, "line_from": 28, "line_to": 41, "context": {"module": "repo", "file_path": "codeqai/repo.py", "file_name": "repo.py", "class_name": null, "function_name": null, "snippet": "def load_files():    git_root = get_git_root(os.getcwd())    file_list = []    for root, dirs, files in os.walk(git_root):        if any(blacklist in root for blacklist in BLACKLIST_DIR):            continue        for file in files:            file_ext = os.path.splitext(file)[1]            if any(whitelist == file_ext for whitelist in WHITELIST_FILES):                if file not in BLACKLIST_FILES:                    file_list.append(os.path.join(root, file))    return file_list"}}
{"name": "get_commit_hash", "signature": "def get_commit_hash(file_path):", "code_type": "Function", "docstring": null, "line": 44, "line_from": 44, "line_to": 60, "context": {"module": "repo", "file_path": "codeqai/repo.py", "file_name": "repo.py", "class_name": null, "function_name": null, "snippet": "def get_commit_hash(file_path):    try:        # Run the git log command        result = subprocess.run(            [\"git\", \"log\", \"-n\", \"1\", \"--pretty=format:%H\", \"--\", file_path],            stdout=subprocess.PIPE,            text=True,            check=True,        )        # Extract the commit hash from the command output        commit_hash = result.stdout.strip()        return commit_hash    except subprocess.CalledProcessError as e:        print(f\"Error executing git command: {e}\")        return None"}}
{"name": "semantic_search", "signature": "def semantic_search(repo_name):", "code_type": "Function", "docstring": null, "line": 13, "line_from": 13, "line_to": 34, "context": {"module": "streamlit", "file_path": "codeqai/streamlit.py", "file_name": "streamlit.py", "class_name": null, "function_name": null, "snippet": "def semantic_search(repo_name: str):    st.title(\"CodeQAI\")    st.subheader(f\"\ud83d\udd0e Semantic search in {repo_name}\")    input = st.text_input(\"Enter a search pattern: \")    if input:        similarity_result = vector_store.similarity_search(input)        for doc in similarity_result:            language = utils.get_programming_language(                utils.get_file_extension(doc.metadata[\"filename\"])            )            start_line, indentation = utils.find_starting_line_and_indent(                doc.metadata[\"filename\"], doc.page_content            )            st.write(doc.metadata[\"filename\"] + \" -> \" + doc.metadata[\"method_name\"])            # TODO add start_line to the code, open PR on streamlit            st.code(                indentation + doc.page_content,                language=language.value,                line_numbers=False,            )"}}
{"name": "stream_response", "signature": "def stream_response(response):", "code_type": "Function", "docstring": null, "line": 38, "line_from": 38, "line_to": 41, "context": {"module": "streamlit", "file_path": "codeqai/streamlit.py", "file_name": "streamlit.py", "class_name": null, "function_name": null, "snippet": "def stream_response(response: str):    for word in response.split():        yield word + \" \"        time.sleep(0.01)"}}
{"name": "chat", "signature": "def chat(memory, repo_name):", "code_type": "Function", "docstring": null, "line": 44, "line_from": 44, "line_to": 75, "context": {"module": "streamlit", "file_path": "codeqai/streamlit.py", "file_name": "streamlit.py", "class_name": null, "function_name": null, "snippet": "def chat(memory: ConversationSummaryMemory, repo_name: str):    st.title(\"CodeQAI\")    st.subheader(f\"\ud83d\udcac Ask anything about the codebase of {repo_name}\")    if st.button(\"Clear Chat\"):        st.session_state[\"chat\"] = []        st.session_state.messages = []        memory.clear()    # Initialize chat history    if \"messages\" not in st.session_state:        st.session_state.messages = []    # Display chat messages from history on app rerun    for message in st.session_state.messages:        with st.chat_message(message[\"role\"]):            st.markdown(message[\"content\"])    # Accept user input    if input := st.chat_input(\"Start chat...\"):        # Add user message to chat history        st.session_state.messages.append({\"role\": \"user\", \"content\": input})        # Display user message in chat message container        with st.chat_message(\"user\"):            st.markdown(input)        # Display assistant response in chat message container        with st.chat_message(\"assistant\"):            result = qa(input)            response = st.write_stream(stream_response(result[\"answer\"]))        # Add assistant response to chat history        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"}}
{"name": "get_programming_language", "signature": "def get_programming_language(file_extension):", "code_type": "Function", "docstring": "Returns the programming language based on the provided file extension.\n\nArgs:\n    file_extension (str): The file extension to determine the programming language of.\n\nReturns:\n    Language: The programming language corresponding to the file extension. If the file extension is not found\n    in the language mapping, returns Language.UNKNOWN.", "line": 9, "line_from": 9, "line_to": 38, "context": {"module": "utils", "file_path": "codeqai/utils.py", "file_name": "utils.py", "class_name": null, "function_name": null, "snippet": "def get_programming_language(file_extension: str) -> Language:    \"\"\"    Returns the programming language based on the provided file extension.    Args:        file_extension (str): The file extension to determine the programming language of.    Returns:        Language: The programming language corresponding to the file extension. If the file extension is not found        in the language mapping, returns Language.UNKNOWN.    \"\"\"    language_mapping = {        \".py\": Language.PYTHON,        \".js\": Language.JAVASCRIPT,        \".jsx\": Language.JAVASCRIPT,        \".mjs\": Language.JAVASCRIPT,        \".cjs\": Language.JAVASCRIPT,        \".ts\": Language.TYPESCRIPT,        \".tsx\": Language.TYPESCRIPT,        \".java\": Language.JAVA,        \".kt\": Language.KOTLIN,        \".rs\": Language.RUST,        \".go\": Language.GO,        \".cpp\": Language.CPP,        \".c\": Language.C,        \".cs\": Language.C_SHARP,        \".hs\": Language.HASKELL,        \".rb\": Language.RUBY,    }    return language_mapping.get(file_extension, Language.UNKNOWN)"}}
{"name": "get_file_extension", "signature": "def get_file_extension(file_name):", "code_type": "Function", "docstring": "Returns the extension of a file from its given name.\n\nParameters:\n    file_name (str): The name of the file.\n\nReturns:\n    str: The extension of the file.", "line": 41, "line_from": 41, "line_to": 52, "context": {"module": "utils", "file_path": "codeqai/utils.py", "file_name": "utils.py", "class_name": null, "function_name": null, "snippet": "def get_file_extension(file_name: str) -> str:    \"\"\"    Returns the extension of a file from its given name.    Parameters:        file_name (str): The name of the file.    Returns:        str: The extension of the file.    \"\"\"    return os.path.splitext(file_name)[-1]"}}
{"name": "get_langchain_language", "signature": "def get_langchain_language(language):", "code_type": "Function", "docstring": null, "line": 55, "line_from": 55, "line_to": 79, "context": {"module": "utils", "file_path": "codeqai/utils.py", "file_name": "utils.py", "class_name": null, "function_name": null, "snippet": "def get_langchain_language(language: Language):    if language == Language.PYTHON:        return text_splitter.Language.PYTHON    elif language == Language.JAVASCRIPT:        return text_splitter.Language.JS    elif language == Language.TYPESCRIPT:        return text_splitter.Language.TS    elif language == Language.JAVA:        return text_splitter.Language.JAVA    elif language == Language.KOTLIN:        return text_splitter.Language.KOTLIN    elif language == Language.RUST:        return text_splitter.Language.RUST    elif language == Language.GO:        return text_splitter.Language.GO    elif language == Language.CPP:        return text_splitter.Language.CPP    elif language == Language.C_SHARP:        return text_splitter.Language.CSHARP    elif language == Language.HASKELL:        return text_splitter.Language.HASKELL    elif language == Language.RUBY:        return text_splitter.Language.RUBY    else:        return None"}}
{"name": "get_bold_text", "signature": "def get_bold_text(text):", "code_type": "Function", "docstring": null, "line": 82, "line_from": 82, "line_to": 83, "context": {"module": "utils", "file_path": "codeqai/utils.py", "file_name": "utils.py", "class_name": null, "function_name": null, "snippet": "def get_bold_text(text):    return f\"\\033[01m{text}\\033[0m\""}}
{"name": "find_starting_line_and_indent", "signature": "def find_starting_line_and_indent(filename, code_snippet):", "code_type": "Function", "docstring": null, "line": 86, "line_from": 86, "line_to": 96, "context": {"module": "utils", "file_path": "codeqai/utils.py", "file_name": "utils.py", "class_name": null, "function_name": null, "snippet": "def find_starting_line_and_indent(filename, code_snippet):    file_path = find_file_in_git_repo(filename)    if file_path is not None:        with open(file_path, \"r\") as file:            file_content = file.read()            start_pos = file_content.find(code_snippet)            return (                file_content.count(\"\\n\", 0, start_pos) + 1,                file_content[:start_pos].split(\"\\n\")[-1],            )    return 1, \"\""}}
{"name": "VectorStore", "signature": "class VectorStore():", "code_type": "Class", "docstring": null, "line": 14, "line_from": 14, "line_to": 178, "context": {"module": "vector_store", "file_path": "codeqai/vector_store.py", "file_name": "vector_store.py", "class_name": "VectorStore", "function_name": null, "snippet": "class VectorStore:    def __init__(self, name: str, embeddings: Embeddings):        self.name = name        self.embeddings = embeddings        self.install_faiss()    def load_documents(self):        with open(            os.path.join(get_cache_path(), f\"{self.name}.faiss.bytes\"), \"rb\"        ) as file:            index = file.read()        self.db = FAISS.deserialize_from_bytes(            embeddings=self.embeddings, serialized=index        )        self.vector_cache = load_vector_cache(f\"{self.name}.json\")        self.retriever = self.db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})    def index_documents(self, documents: list[Document]):        self.vector_cache = {}        self.db = FAISS.from_documents(documents, self.embeddings)        index = self.db.serialize_to_bytes()        with open(            os.path.join(get_cache_path(), f\"{self.name}.faiss.bytes\"), \"wb\"        ) as binary_file:            binary_file.write(index)        # Create vector cache        index_to_docstore_id = self.db.index_to_docstore_id        for i in range(len(documents)):            document = self.db.docstore.search(index_to_docstore_id[i])            if document:                # Check if the document is already present in the vector cache                # if yes, then add the vector id to the vector cache entry                if self.vector_cache.get(document.metadata[\"filename\"]):                    self.vector_cache[document.metadata[\"filename\"]].vector_ids.append(                        index_to_docstore_id[i]                    )                # if no, then create a new entry in the vector cache                else:                    self.vector_cache[document.metadata[\"filename\"]] = VectorCache(                        document.metadata[\"filename\"],                        [index_to_docstore_id[i]],                        document.metadata[\"commit_hash\"],                    )        self.retriever = self.db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})    def sync_documents(self, files):        new_filenames = set()        for file in files:            filename = os.path.basename(file)            new_filenames.add(filename)            commit_hash = get_commit_hash(file)            # Check if the document is already present in the vector cache            # if yes, then check if the document has been modified or not            if filename in self.vector_cache:                # Check if the document has been modified, if yes delete all old vectors and add new vector                if self.vector_cache[filename].commit_hash != commit_hash:                    # This will delete all the vectors associated with the document                    # incluing db.index_to_docstore_id, db.docstore and db.index                    try:                        self.db.delete(self.vector_cache[filename].vector_ids)                    except Exception as e:                        print(f\"Error deleting vectors for file {filename}: {e}\")                    # Add the new document to the vector store and recreate the vector cache entry                    self.vector_cache[filename] = VectorCache(                        filename,                        [],                        commit_hash,                    )                    documents = parse_code_files([file])                    for document in documents:                        self.db.add_documents([document])                        self.vector_cache[filename].vector_ids.append(                            self.db.index_to_docstore_id[                                len(self.db.index_to_docstore_id) - 1                            ]                        )            # if no, then create a new entry in the vector cache and add the document to the vector store            else:                self.vector_cache[filename] = VectorCache(                    filename,                    [],                    commit_hash,                )                documents = parse_code_files([file])                for document in documents:                    self.db.add_documents([document])                    self.vector_cache[filename].vector_ids.append(                        self.db.index_to_docstore_id[                            len(self.db.index_to_docstore_id) - 1                        ]                    )        # Remove old documents from the vector store        deleted_files = []        for cache_item in self.vector_cache.values():            if cache_item.filename not in new_filenames:                try:                    self.db.delete(cache_item.vector_ids)                except Exception as e:                    print(f\"Error deleting vectors for file {cache_item.filename}: {e}\")                deleted_files.append(cache_item.filename)        # Remove old filenames from the vector cache        for deleted_file in deleted_files:            self.vector_cache.pop(deleted_file)        index = self.db.serialize_to_bytes()        with open(            os.path.join(get_cache_path(), f\"{self.name}.faiss.bytes\"), \"wb\"        ) as binary_file:            binary_file.write(index)    def similarity_search(self, query: str):        return self.db.similarity_search(query, k=4)    def install_faiss(self):        try:            import faiss  # noqa: F401        except ImportError:            question = [                inquirer.Confirm(                    \"confirm\",                    message=f\"{utils.get_bold_text('faiss')} package not found in this python environment. Do you want to install it now?\",                    default=True,                ),            ]            answers = inquirer.prompt(question)            if answers and answers[\"confirm\"]:                import subprocess                import sys                question = [                    inquirer.List(                        \"faiss-installation\",                        message=\"Please select the appropriate option to install FAISS. Use gpu if your system supports CUDA\",                        choices=[                            \"faiss-cpu\",                            \"faiss-gpu\",                        ],                        default=\"faiss-cpu\",                    ),                ]                answers = inquirer.prompt(question)                if answers and answers[\"faiss-installation\"]:                    try:                        subprocess.run(                            [                                sys.executable,                                \"-m\",                                \"pip\",                                \"install\",                                answers[\"faiss-installation\"],                            ],                            check=True,                        )                    except subprocess.CalledProcessError as e:                        print(f\"Error during faiss installation: {e}\")            else:                exit(\"faiss package is required for codeqai to work.\")"}}
{"name": "__init__", "signature": "def __init__(self, name, embeddings):", "code_type": "Function", "docstring": null, "line": 15, "line_from": 15, "line_to": 18, "context": {"module": "vector_store", "file_path": "codeqai/vector_store.py", "file_name": "vector_store.py", "class_name": "VectorStore", "function_name": null, "snippet": "def __init__(self, name: str, embeddings: Embeddings):        self.name = name        self.embeddings = embeddings        self.install_faiss()"}}
{"name": "load_documents", "signature": "def load_documents(self):", "code_type": "Function", "docstring": null, "line": 20, "line_from": 20, "line_to": 30, "context": {"module": "vector_store", "file_path": "codeqai/vector_store.py", "file_name": "vector_store.py", "class_name": "VectorStore", "function_name": null, "snippet": "def load_documents(self):        with open(            os.path.join(get_cache_path(), f\"{self.name}.faiss.bytes\"), \"rb\"        ) as file:            index = file.read()        self.db = FAISS.deserialize_from_bytes(            embeddings=self.embeddings, serialized=index        )        self.vector_cache = load_vector_cache(f\"{self.name}.json\")        self.retriever = self.db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})"}}
{"name": "index_documents", "signature": "def index_documents(self, documents):", "code_type": "Function", "docstring": null, "line": 32, "line_from": 32, "line_to": 59, "context": {"module": "vector_store", "file_path": "codeqai/vector_store.py", "file_name": "vector_store.py", "class_name": "VectorStore", "function_name": null, "snippet": "def index_documents(self, documents: list[Document]):        self.vector_cache = {}        self.db = FAISS.from_documents(documents, self.embeddings)        index = self.db.serialize_to_bytes()        with open(            os.path.join(get_cache_path(), f\"{self.name}.faiss.bytes\"), \"wb\"        ) as binary_file:            binary_file.write(index)        # Create vector cache        index_to_docstore_id = self.db.index_to_docstore_id        for i in range(len(documents)):            document = self.db.docstore.search(index_to_docstore_id[i])            if document:                # Check if the document is already present in the vector cache                # if yes, then add the vector id to the vector cache entry                if self.vector_cache.get(document.metadata[\"filename\"]):                    self.vector_cache[document.metadata[\"filename\"]].vector_ids.append(                        index_to_docstore_id[i]                    )                # if no, then create a new entry in the vector cache                else:                    self.vector_cache[document.metadata[\"filename\"]] = VectorCache(                        document.metadata[\"filename\"],                        [index_to_docstore_id[i]],                        document.metadata[\"commit_hash\"],                    )        self.retriever = self.db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 8})"}}
{"name": "sync_documents", "signature": "def sync_documents(self, files):", "code_type": "Function", "docstring": null, "line": 61, "line_from": 61, "line_to": 128, "context": {"module": "vector_store", "file_path": "codeqai/vector_store.py", "file_name": "vector_store.py", "class_name": "VectorStore", "function_name": null, "snippet": "def sync_documents(self, files):        new_filenames = set()        for file in files:            filename = os.path.basename(file)            new_filenames.add(filename)            commit_hash = get_commit_hash(file)            # Check if the document is already present in the vector cache            # if yes, then check if the document has been modified or not            if filename in self.vector_cache:                # Check if the document has been modified, if yes delete all old vectors and add new vector                if self.vector_cache[filename].commit_hash != commit_hash:                    # This will delete all the vectors associated with the document                    # incluing db.index_to_docstore_id, db.docstore and db.index                    try:                        self.db.delete(self.vector_cache[filename].vector_ids)                    except Exception as e:                        print(f\"Error deleting vectors for file {filename}: {e}\")                    # Add the new document to the vector store and recreate the vector cache entry                    self.vector_cache[filename] = VectorCache(                        filename,                        [],                        commit_hash,                    )                    documents = parse_code_files([file])                    for document in documents:                        self.db.add_documents([document])                        self.vector_cache[filename].vector_ids.append(                            self.db.index_to_docstore_id[                                len(self.db.index_to_docstore_id) - 1                            ]                        )            # if no, then create a new entry in the vector cache and add the document to the vector store            else:                self.vector_cache[filename] = VectorCache(                    filename,                    [],                    commit_hash,                )                documents = parse_code_files([file])                for document in documents:                    self.db.add_documents([document])                    self.vector_cache[filename].vector_ids.append(                        self.db.index_to_docstore_id[                            len(self.db.index_to_docstore_id) - 1                        ]                    )        # Remove old documents from the vector store        deleted_files = []        for cache_item in self.vector_cache.values():            if cache_item.filename not in new_filenames:                try:                    self.db.delete(cache_item.vector_ids)                except Exception as e:                    print(f\"Error deleting vectors for file {cache_item.filename}: {e}\")                deleted_files.append(cache_item.filename)        # Remove old filenames from the vector cache        for deleted_file in deleted_files:            self.vector_cache.pop(deleted_file)        index = self.db.serialize_to_bytes()        with open(            os.path.join(get_cache_path(), f\"{self.name}.faiss.bytes\"), \"wb\"        ) as binary_file:            binary_file.write(index)"}}
{"name": "similarity_search", "signature": "def similarity_search(self, query):", "code_type": "Function", "docstring": null, "line": 130, "line_from": 130, "line_to": 131, "context": {"module": "vector_store", "file_path": "codeqai/vector_store.py", "file_name": "vector_store.py", "class_name": "VectorStore", "function_name": null, "snippet": "def similarity_search(self, query: str):        return self.db.similarity_search(query, k=4)"}}
{"name": "install_faiss", "signature": "def install_faiss(self):", "code_type": "Function", "docstring": null, "line": 133, "line_from": 133, "line_to": 178, "context": {"module": "vector_store", "file_path": "codeqai/vector_store.py", "file_name": "vector_store.py", "class_name": "VectorStore", "function_name": null, "snippet": "def install_faiss(self):        try:            import faiss  # noqa: F401        except ImportError:            question = [                inquirer.Confirm(                    \"confirm\",                    message=f\"{utils.get_bold_text('faiss')} package not found in this python environment. Do you want to install it now?\",                    default=True,                ),            ]            answers = inquirer.prompt(question)            if answers and answers[\"confirm\"]:                import subprocess                import sys                question = [                    inquirer.List(                        \"faiss-installation\",                        message=\"Please select the appropriate option to install FAISS. Use gpu if your system supports CUDA\",                        choices=[                            \"faiss-cpu\",                            \"faiss-gpu\",                        ],                        default=\"faiss-cpu\",                    ),                ]                answers = inquirer.prompt(question)                if answers and answers[\"faiss-installation\"]:                    try:                        subprocess.run(                            [                                sys.executable,                                \"-m\",                                \"pip\",                                \"install\",                                answers[\"faiss-installation\"],                            ],                            check=True,                        )                    except subprocess.CalledProcessError as e:                        print(f\"Error during faiss installation: {e}\")            else:                exit(\"faiss package is required for codeqai to work.\")"}}
{"name": "main", "signature": "def main():", "code_type": "Function", "docstring": null, "line": 4, "line_from": 4, "line_to": 5, "context": {"module": "__main__", "file_path": "codeqai/__main__.py", "file_name": "__main__.py", "class_name": null, "function_name": null, "snippet": "def main():    app.run()"}}
{"name": "TreesitterMethodNode", "signature": "class TreesitterMethodNode():", "code_type": "Class", "docstring": null, "line": 10, "line_from": 10, "line_to": 21, "context": {"module": "treesitter", "file_path": "codeqai/treesitter/treesitter.py", "file_name": "treesitter.py", "class_name": "TreesitterMethodNode", "function_name": null, "snippet": "class TreesitterMethodNode:    def __init__(        self,        name: \"str | bytes | None\",        doc_comment: \"str | None\",        method_source_code: \"str | None\",        node: tree_sitter.Node,    ):        self.name = name        self.doc_comment = doc_comment        self.method_source_code = method_source_code or node.text.decode()        self.node = node"}}
{"name": "__init__", "signature": "def __init__(self, name, doc_comment, method_source_code, node):", "code_type": "Function", "docstring": null, "line": 11, "line_from": 11, "line_to": 21, "context": {"module": "treesitter", "file_path": "codeqai/treesitter/treesitter.py", "file_name": "treesitter.py", "class_name": "TreesitterMethodNode", "function_name": null, "snippet": "def __init__(        self,        name: \"str | bytes | None\",        doc_comment: \"str | None\",        method_source_code: \"str | None\",        node: tree_sitter.Node,    ):        self.name = name        self.doc_comment = doc_comment        self.method_source_code = method_source_code or node.text.decode()        self.node = node"}}
{"name": "Treesitter", "signature": "class Treesitter(ABC):", "code_type": "Class", "docstring": null, "line": 24, "line_from": 24, "line_to": 77, "context": {"module": "treesitter", "file_path": "codeqai/treesitter/treesitter.py", "file_name": "treesitter.py", "class_name": "Treesitter", "function_name": null, "snippet": "class Treesitter(ABC):    def __init__(        self,        language: Language,        method_declaration_identifier: str,        name_identifier: str,        doc_comment_identifier: str,    ):        self.parser = get_parser(language.value)        self.language = get_language(language.value)        self.method_declaration_identifier = method_declaration_identifier        self.method_name_identifier = name_identifier        self.doc_comment_identifier = doc_comment_identifier    @staticmethod    def create_treesitter(language: Language) -> \"Treesitter\":        return TreesitterRegistry.create_treesitter(language)    def parse(self, file_bytes: bytes) -> list[TreesitterMethodNode]:        self.tree = self.parser.parse(file_bytes)        result = []        methods = self._query_all_methods(self.tree.root_node)        for method in methods:            method_name = self._query_method_name(method[\"method\"])            doc_comment = method[\"doc_comment\"]            result.append(                TreesitterMethodNode(method_name, doc_comment, None, method[\"method\"])            )        return result    def _query_all_methods(        self,        node: tree_sitter.Node,    ):        methods = []        if node.type == self.method_declaration_identifier:            doc_comment_node = None            if (                node.prev_named_sibling                and node.prev_named_sibling.type == self.doc_comment_identifier            ):                doc_comment_node = node.prev_named_sibling.text.decode()            methods.append({\"method\": node, \"doc_comment\": doc_comment_node})        else:            for child in node.children:                methods.extend(self._query_all_methods(child))        return methods    def _query_method_name(self, node: tree_sitter.Node):        if node.type == self.method_declaration_identifier:            for child in node.children:                if child.type == self.method_name_identifier:                    return child.text.decode()        return None"}}
{"name": "__init__", "signature": "def __init__(self, language, method_declaration_identifier, name_identifier, doc_comment_identifier):", "code_type": "Function", "docstring": null, "line": 25, "line_from": 25, "line_to": 36, "context": {"module": "treesitter", "file_path": "codeqai/treesitter/treesitter.py", "file_name": "treesitter.py", "class_name": "Treesitter", "function_name": null, "snippet": "def __init__(        self,        language: Language,        method_declaration_identifier: str,        name_identifier: str,        doc_comment_identifier: str,    ):        self.parser = get_parser(language.value)        self.language = get_language(language.value)        self.method_declaration_identifier = method_declaration_identifier        self.method_name_identifier = name_identifier        self.doc_comment_identifier = doc_comment_identifier"}}
{"name": "create_treesitter", "signature": "def create_treesitter(language):", "code_type": "Function", "docstring": null, "line": 39, "line_from": 39, "line_to": 40, "context": {"module": "treesitter", "file_path": "codeqai/treesitter/treesitter.py", "file_name": "treesitter.py", "class_name": "Treesitter", "function_name": null, "snippet": "def create_treesitter(language: Language) -> \"Treesitter\":        return TreesitterRegistry.create_treesitter(language)"}}
{"name": "parse", "signature": "def parse(self, file_bytes):", "code_type": "Function", "docstring": null, "line": 42, "line_from": 42, "line_to": 52, "context": {"module": "treesitter", "file_path": "codeqai/treesitter/treesitter.py", "file_name": "treesitter.py", "class_name": "Treesitter", "function_name": null, "snippet": "def parse(self, file_bytes: bytes) -> list[TreesitterMethodNode]:        self.tree = self.parser.parse(file_bytes)        result = []        methods = self._query_all_methods(self.tree.root_node)        for method in methods:            method_name = self._query_method_name(method[\"method\"])            doc_comment = method[\"doc_comment\"]            result.append(                TreesitterMethodNode(method_name, doc_comment, None, method[\"method\"])            )        return result"}}
{"name": "_query_all_methods", "signature": "def _query_all_methods(self, node):", "code_type": "Function", "docstring": null, "line": 54, "line_from": 54, "line_to": 70, "context": {"module": "treesitter", "file_path": "codeqai/treesitter/treesitter.py", "file_name": "treesitter.py", "class_name": "Treesitter", "function_name": null, "snippet": "def _query_all_methods(        self,        node: tree_sitter.Node,    ):        methods = []        if node.type == self.method_declaration_identifier:            doc_comment_node = None            if (                node.prev_named_sibling                and node.prev_named_sibling.type == self.doc_comment_identifier            ):                doc_comment_node = node.prev_named_sibling.text.decode()            methods.append({\"method\": node, \"doc_comment\": doc_comment_node})        else:            for child in node.children:                methods.extend(self._query_all_methods(child))        return methods"}}
{"name": "_query_method_name", "signature": "def _query_method_name(self, node):", "code_type": "Function", "docstring": null, "line": 72, "line_from": 72, "line_to": 77, "context": {"module": "treesitter", "file_path": "codeqai/treesitter/treesitter.py", "file_name": "treesitter.py", "class_name": "Treesitter", "function_name": null, "snippet": "def _query_method_name(self, node: tree_sitter.Node):        if node.type == self.method_declaration_identifier:            for child in node.children:                if child.type == self.method_name_identifier:                    return child.text.decode()        return None"}}
{"name": "TreesitterC", "signature": "class TreesitterC(Treesitter):", "code_type": "Class", "docstring": null, "line": 8, "line_from": 8, "line_to": 22, "context": {"module": "treesitter_c", "file_path": "codeqai/treesitter/treesitter_c.py", "file_name": "treesitter_c.py", "class_name": "TreesitterC", "function_name": null, "snippet": "class TreesitterC(Treesitter):    def __init__(self):        super().__init__(Language.C, \"function_definition\", \"identifier\", \"comment\")    def _query_method_name(self, node: tree_sitter.Node):        if node.type == self.method_declaration_identifier:            for child in node.children:                # if method returns pointer, skip pointer declarator                if child.type == \"pointer_declarator\":                    child = child.children[1]                if child.type == \"function_declarator\":                    for child in child.children:                        if child.type == self.method_name_identifier:                            return child.text.decode()        return None"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 9, "line_from": 9, "line_to": 10, "context": {"module": "treesitter_c", "file_path": "codeqai/treesitter/treesitter_c.py", "file_name": "treesitter_c.py", "class_name": "TreesitterC", "function_name": null, "snippet": "def __init__(self):        super().__init__(Language.C, \"function_definition\", \"identifier\", \"comment\")"}}
{"name": "_query_method_name", "signature": "def _query_method_name(self, node):", "code_type": "Function", "docstring": null, "line": 12, "line_from": 12, "line_to": 22, "context": {"module": "treesitter_c", "file_path": "codeqai/treesitter/treesitter_c.py", "file_name": "treesitter_c.py", "class_name": "TreesitterC", "function_name": null, "snippet": "def _query_method_name(self, node: tree_sitter.Node):        if node.type == self.method_declaration_identifier:            for child in node.children:                # if method returns pointer, skip pointer declarator                if child.type == \"pointer_declarator\":                    child = child.children[1]                if child.type == \"function_declarator\":                    for child in child.children:                        if child.type == self.method_name_identifier:                            return child.text.decode()        return None"}}
{"name": "TreesitterCpp", "signature": "class TreesitterCpp(Treesitter):", "code_type": "Class", "docstring": null, "line": 8, "line_from": 8, "line_to": 22, "context": {"module": "treesitter_cpp", "file_path": "codeqai/treesitter/treesitter_cpp.py", "file_name": "treesitter_cpp.py", "class_name": "TreesitterCpp", "function_name": null, "snippet": "class TreesitterCpp(Treesitter):    def __init__(self):        super().__init__(Language.CPP, \"function_definition\", \"identifier\", \"comment\")    def _query_method_name(self, node: tree_sitter.Node):        if node.type == self.method_declaration_identifier:            for child in node.children:                # if method returns pointer, skip pointer declarator                if child.type == \"pointer_declarator\":                    child = child.children[1]                if child.type == \"function_declarator\":                    for child in child.children:                        if child.type == self.method_name_identifier:                            return child.text.decode()        return None"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 9, "line_from": 9, "line_to": 10, "context": {"module": "treesitter_cpp", "file_path": "codeqai/treesitter/treesitter_cpp.py", "file_name": "treesitter_cpp.py", "class_name": "TreesitterCpp", "function_name": null, "snippet": "def __init__(self):        super().__init__(Language.CPP, \"function_definition\", \"identifier\", \"comment\")"}}
{"name": "_query_method_name", "signature": "def _query_method_name(self, node):", "code_type": "Function", "docstring": null, "line": 12, "line_from": 12, "line_to": 22, "context": {"module": "treesitter_cpp", "file_path": "codeqai/treesitter/treesitter_cpp.py", "file_name": "treesitter_cpp.py", "class_name": "TreesitterCpp", "function_name": null, "snippet": "def _query_method_name(self, node: tree_sitter.Node):        if node.type == self.method_declaration_identifier:            for child in node.children:                # if method returns pointer, skip pointer declarator                if child.type == \"pointer_declarator\":                    child = child.children[1]                if child.type == \"function_declarator\":                    for child in child.children:                        if child.type == self.method_name_identifier:                            return child.text.decode()        return None"}}
{"name": "TreesitterCsharp", "signature": "class TreesitterCsharp(Treesitter):", "code_type": "Class", "docstring": null, "line": 8, "line_from": 8, "line_to": 58, "context": {"module": "treesitter_cs", "file_path": "codeqai/treesitter/treesitter_cs.py", "file_name": "treesitter_cs.py", "class_name": "TreesitterCsharp", "function_name": null, "snippet": "class TreesitterCsharp(Treesitter):    def __init__(self):        super().__init__(            Language.C_SHARP, \"method_declaration\", \"identifier\", \"comment\"        )    def _query_method_name(self, node: tree_sitter.Node):        first_match = None        if node.type == self.method_declaration_identifier:            for child in node.children:                # if the return type is an object type, then the method name                # is the second match                if child.type == self.method_name_identifier and not first_match:                    first_match = child.text.decode()                elif child.type == self.method_name_identifier and first_match:                    return child.text.decode()        return first_match    def _query_all_methods(self, node: tree_sitter.Node):        methods = []        if node.type == self.method_declaration_identifier:            doc_comment_nodes = []            if (                node.prev_named_sibling                and node.prev_named_sibling.type == self.doc_comment_identifier            ):                current_doc_comment_node = node.prev_named_sibling                while (                    current_doc_comment_node                    and current_doc_comment_node.type == self.doc_comment_identifier                ):                    doc_comment_nodes.append(current_doc_comment_node.text.decode())                    if current_doc_comment_node.prev_named_sibling:                        current_doc_comment_node = (                            current_doc_comment_node.prev_named_sibling                        )                    else:                        current_doc_comment_node = None            doc_comment_str = \"\"            doc_comment_nodes.reverse()            for doc_comment_node in doc_comment_nodes:                doc_comment_str += doc_comment_node + \"\\n\"            if doc_comment_str.strip() != \"\":                methods.append({\"method\": node, \"doc_comment\": doc_comment_str.strip()})            else:                methods.append({\"method\": node, \"doc_comment\": None})        else:            for child in node.children:                methods.extend(self._query_all_methods(child))        return methods"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 9, "line_from": 9, "line_to": 12, "context": {"module": "treesitter_cs", "file_path": "codeqai/treesitter/treesitter_cs.py", "file_name": "treesitter_cs.py", "class_name": "TreesitterCsharp", "function_name": null, "snippet": "def __init__(self):        super().__init__(            Language.C_SHARP, \"method_declaration\", \"identifier\", \"comment\"        )"}}
{"name": "_query_method_name", "signature": "def _query_method_name(self, node):", "code_type": "Function", "docstring": null, "line": 14, "line_from": 14, "line_to": 24, "context": {"module": "treesitter_cs", "file_path": "codeqai/treesitter/treesitter_cs.py", "file_name": "treesitter_cs.py", "class_name": "TreesitterCsharp", "function_name": null, "snippet": "def _query_method_name(self, node: tree_sitter.Node):        first_match = None        if node.type == self.method_declaration_identifier:            for child in node.children:                # if the return type is an object type, then the method name                # is the second match                if child.type == self.method_name_identifier and not first_match:                    first_match = child.text.decode()                elif child.type == self.method_name_identifier and first_match:                    return child.text.decode()        return first_match"}}
{"name": "_query_all_methods", "signature": "def _query_all_methods(self, node):", "code_type": "Function", "docstring": null, "line": 26, "line_from": 26, "line_to": 58, "context": {"module": "treesitter_cs", "file_path": "codeqai/treesitter/treesitter_cs.py", "file_name": "treesitter_cs.py", "class_name": "TreesitterCsharp", "function_name": null, "snippet": "def _query_all_methods(self, node: tree_sitter.Node):        methods = []        if node.type == self.method_declaration_identifier:            doc_comment_nodes = []            if (                node.prev_named_sibling                and node.prev_named_sibling.type == self.doc_comment_identifier            ):                current_doc_comment_node = node.prev_named_sibling                while (                    current_doc_comment_node                    and current_doc_comment_node.type == self.doc_comment_identifier                ):                    doc_comment_nodes.append(current_doc_comment_node.text.decode())                    if current_doc_comment_node.prev_named_sibling:                        current_doc_comment_node = (                            current_doc_comment_node.prev_named_sibling                        )                    else:                        current_doc_comment_node = None            doc_comment_str = \"\"            doc_comment_nodes.reverse()            for doc_comment_node in doc_comment_nodes:                doc_comment_str += doc_comment_node + \"\\n\"            if doc_comment_str.strip() != \"\":                methods.append({\"method\": node, \"doc_comment\": doc_comment_str.strip()})            else:                methods.append({\"method\": node, \"doc_comment\": None})        else:            for child in node.children:                methods.extend(self._query_all_methods(child))        return methods"}}
{"name": "TreesitterGo", "signature": "class TreesitterGo(Treesitter):", "code_type": "Class", "docstring": null, "line": 6, "line_from": 6, "line_to": 8, "context": {"module": "treesitter_go", "file_path": "codeqai/treesitter/treesitter_go.py", "file_name": "treesitter_go.py", "class_name": "TreesitterGo", "function_name": null, "snippet": "class TreesitterGo(Treesitter):    def __init__(self):        super().__init__(Language.GO, \"function_declaration\", \"identifier\", \"comment\")"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 7, "line_from": 7, "line_to": 8, "context": {"module": "treesitter_go", "file_path": "codeqai/treesitter/treesitter_go.py", "file_name": "treesitter_go.py", "class_name": "TreesitterGo", "function_name": null, "snippet": "def __init__(self):        super().__init__(Language.GO, \"function_declaration\", \"identifier\", \"comment\")"}}
{"name": "TreesitterHaskell", "signature": "class TreesitterHaskell(Treesitter):", "code_type": "Class", "docstring": null, "line": 10, "line_from": 10, "line_to": 86, "context": {"module": "treesitter_hs", "file_path": "codeqai/treesitter/treesitter_hs.py", "file_name": "treesitter_hs.py", "class_name": "TreesitterHaskell", "function_name": null, "snippet": "class TreesitterHaskell(Treesitter):    def __init__(self):        super().__init__(Language.HASKELL, \"function\", \"variable\", \"comment\")    def parse(self, file_bytes: bytes) -> list[TreesitterMethodNode]:        self.tree = self.parser.parse(file_bytes)        result = []        methods = self._query_all_methods(self.tree.root_node)        for method in methods:            method_name = self._query_method_name(method[\"method\"])            doc_comment = method[\"doc_comment\"]            source_code = None            if method[\"method\"].type == \"signature\":                sc = map(                    lambda x: \"\\n\" + x.text.decode() if x.type == \"function\" else \"\",                    method[\"method\"].children,                )                source_code = method[\"method\"].text.decode() + \"\".join(sc)            result.append(                TreesitterMethodNode(                    method_name, doc_comment, source_code, method[\"method\"]                )            )        return result    def _query_all_methods(        self,        node: tree_sitter.Node,    ):        methods = []        if node.type == self.method_declaration_identifier:            doc_comment_node = None            if (                node.prev_named_sibling                and node.prev_named_sibling.type == self.doc_comment_identifier            ):                doc_comment_node = node.prev_named_sibling.text.decode()            else:                if (                    node.prev_named_sibling                    and node.prev_named_sibling.type == \"signature\"                ):                    prev_node = node.prev_named_sibling                    if (                        prev_node.prev_named_sibling                        and prev_node.prev_named_sibling.type                        == self.doc_comment_identifier                    ):                        doc_comment_node = prev_node.prev_named_sibling.text.decode()                    prev_node.children.append(node)                    node = prev_node            methods.append({\"method\": node, \"doc_comment\": doc_comment_node})        else:            for child in node.children:                current = self._query_all_methods(child)                if methods and current:                    previous = methods[-1]                    if self._query_method_name(                        previous[\"method\"]                    ) == self._query_method_name(current[0][\"method\"]):                        previous[\"method\"].children.extend(                            map(lambda x: x[\"method\"], current)                        )                        methods = methods[:-1]                        methods.append(previous)                    else:                        methods.extend(current)                else:                    methods.extend(current)        return methods    def _query_method_name(self, node: tree_sitter.Node):        if node.type == \"signature\" or node.type == self.method_declaration_identifier:            for child in node.children:                if child.type == self.method_name_identifier:                    return child.text.decode()        return None"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 11, "line_from": 11, "line_to": 12, "context": {"module": "treesitter_hs", "file_path": "codeqai/treesitter/treesitter_hs.py", "file_name": "treesitter_hs.py", "class_name": "TreesitterHaskell", "function_name": null, "snippet": "def __init__(self):        super().__init__(Language.HASKELL, \"function\", \"variable\", \"comment\")"}}
{"name": "parse", "signature": "def parse(self, file_bytes):", "code_type": "Function", "docstring": null, "line": 14, "line_from": 14, "line_to": 33, "context": {"module": "treesitter_hs", "file_path": "codeqai/treesitter/treesitter_hs.py", "file_name": "treesitter_hs.py", "class_name": "TreesitterHaskell", "function_name": null, "snippet": "def parse(self, file_bytes: bytes) -> list[TreesitterMethodNode]:        self.tree = self.parser.parse(file_bytes)        result = []        methods = self._query_all_methods(self.tree.root_node)        for method in methods:            method_name = self._query_method_name(method[\"method\"])            doc_comment = method[\"doc_comment\"]            source_code = None            if method[\"method\"].type == \"signature\":                sc = map(                    lambda x: \"\\n\" + x.text.decode() if x.type == \"function\" else \"\",                    method[\"method\"].children,                )                source_code = method[\"method\"].text.decode() + \"\".join(sc)            result.append(                TreesitterMethodNode(                    method_name, doc_comment, source_code, method[\"method\"]                )            )        return result"}}
{"name": "_query_all_methods", "signature": "def _query_all_methods(self, node):", "code_type": "Function", "docstring": null, "line": 35, "line_from": 35, "line_to": 79, "context": {"module": "treesitter_hs", "file_path": "codeqai/treesitter/treesitter_hs.py", "file_name": "treesitter_hs.py", "class_name": "TreesitterHaskell", "function_name": null, "snippet": "def _query_all_methods(        self,        node: tree_sitter.Node,    ):        methods = []        if node.type == self.method_declaration_identifier:            doc_comment_node = None            if (                node.prev_named_sibling                and node.prev_named_sibling.type == self.doc_comment_identifier            ):                doc_comment_node = node.prev_named_sibling.text.decode()            else:                if (                    node.prev_named_sibling                    and node.prev_named_sibling.type == \"signature\"                ):                    prev_node = node.prev_named_sibling                    if (                        prev_node.prev_named_sibling                        and prev_node.prev_named_sibling.type                        == self.doc_comment_identifier                    ):                        doc_comment_node = prev_node.prev_named_sibling.text.decode()                    prev_node.children.append(node)                    node = prev_node            methods.append({\"method\": node, \"doc_comment\": doc_comment_node})        else:            for child in node.children:                current = self._query_all_methods(child)                if methods and current:                    previous = methods[-1]                    if self._query_method_name(                        previous[\"method\"]                    ) == self._query_method_name(current[0][\"method\"]):                        previous[\"method\"].children.extend(                            map(lambda x: x[\"method\"], current)                        )                        methods = methods[:-1]                        methods.append(previous)                    else:                        methods.extend(current)                else:                    methods.extend(current)        return methods"}}
{"name": "_query_method_name", "signature": "def _query_method_name(self, node):", "code_type": "Function", "docstring": null, "line": 81, "line_from": 81, "line_to": 86, "context": {"module": "treesitter_hs", "file_path": "codeqai/treesitter/treesitter_hs.py", "file_name": "treesitter_hs.py", "class_name": "TreesitterHaskell", "function_name": null, "snippet": "def _query_method_name(self, node: tree_sitter.Node):        if node.type == \"signature\" or node.type == self.method_declaration_identifier:            for child in node.children:                if child.type == self.method_name_identifier:                    return child.text.decode()        return None"}}
{"name": "TreesitterJava", "signature": "class TreesitterJava(Treesitter):", "code_type": "Class", "docstring": null, "line": 6, "line_from": 6, "line_to": 10, "context": {"module": "treesitter_java", "file_path": "codeqai/treesitter/treesitter_java.py", "file_name": "treesitter_java.py", "class_name": "TreesitterJava", "function_name": null, "snippet": "class TreesitterJava(Treesitter):    def __init__(self):        super().__init__(            Language.JAVA, \"method_declaration\", \"identifier\", \"block_comment\"        )"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 7, "line_from": 7, "line_to": 10, "context": {"module": "treesitter_java", "file_path": "codeqai/treesitter/treesitter_java.py", "file_name": "treesitter_java.py", "class_name": "TreesitterJava", "function_name": null, "snippet": "def __init__(self):        super().__init__(            Language.JAVA, \"method_declaration\", \"identifier\", \"block_comment\"        )"}}
{"name": "TreesitterJavascript", "signature": "class TreesitterJavascript(Treesitter):", "code_type": "Class", "docstring": null, "line": 6, "line_from": 6, "line_to": 10, "context": {"module": "treesitter_js", "file_path": "codeqai/treesitter/treesitter_js.py", "file_name": "treesitter_js.py", "class_name": "TreesitterJavascript", "function_name": null, "snippet": "class TreesitterJavascript(Treesitter):    def __init__(self):        super().__init__(            Language.JAVASCRIPT, \"function_declaration\", \"identifier\", \"comment\"        )"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 7, "line_from": 7, "line_to": 10, "context": {"module": "treesitter_js", "file_path": "codeqai/treesitter/treesitter_js.py", "file_name": "treesitter_js.py", "class_name": "TreesitterJavascript", "function_name": null, "snippet": "def __init__(self):        super().__init__(            Language.JAVASCRIPT, \"function_declaration\", \"identifier\", \"comment\"        )"}}
{"name": "TreesitterKotlin", "signature": "class TreesitterKotlin(Treesitter):", "code_type": "Class", "docstring": null, "line": 6, "line_from": 6, "line_to": 10, "context": {"module": "treesitter_kt", "file_path": "codeqai/treesitter/treesitter_kt.py", "file_name": "treesitter_kt.py", "class_name": "TreesitterKotlin", "function_name": null, "snippet": "class TreesitterKotlin(Treesitter):    def __init__(self):        super().__init__(            Language.KOTLIN, \"function_declaration\", \"simple_identifier\", \"comment\"        )"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 7, "line_from": 7, "line_to": 10, "context": {"module": "treesitter_kt", "file_path": "codeqai/treesitter/treesitter_kt.py", "file_name": "treesitter_kt.py", "class_name": "TreesitterKotlin", "function_name": null, "snippet": "def __init__(self):        super().__init__(            Language.KOTLIN, \"function_declaration\", \"simple_identifier\", \"comment\"        )"}}
{"name": "TreesitterPython", "signature": "class TreesitterPython(Treesitter):", "code_type": "Class", "docstring": null, "line": 8, "line_from": 8, "line_to": 54, "context": {"module": "treesitter_py", "file_path": "codeqai/treesitter/treesitter_py.py", "file_name": "treesitter_py.py", "class_name": "TreesitterPython", "function_name": null, "snippet": "class TreesitterPython(Treesitter):    def __init__(self):        super().__init__(            Language.PYTHON, \"function_definition\", \"identifier\", \"expression_statement\"        )    def parse(self, file_bytes: bytes) -> list[TreesitterMethodNode]:        self.tree = self.parser.parse(file_bytes)        result = []        methods = self._query_all_methods(self.tree.root_node)        for method in methods:            method_name = self._query_method_name(method)            doc_comment = self._query_doc_comment(method)            result.append(TreesitterMethodNode(method_name, doc_comment, None, method))        return result    def _query_method_name(self, node: tree_sitter.Node):        if node.type == self.method_declaration_identifier:            for child in node.children:                if child.type == self.method_name_identifier:                    return child.text.decode()        return None    def _query_all_methods(self, node: tree_sitter.Node):        methods = []        for child in node.children:            if child.type == self.method_declaration_identifier:                methods.append(child)            if child.type == \"class_definition\":                class_body = child.children[-1]                for child_node in class_body.children:                    if child_node.type == self.method_declaration_identifier:                        methods.append(child_node)        return methods    def _query_doc_comment(self, node: tree_sitter.Node):        query_code = \"\"\"            (function_definition                body: (block . (expression_statement (string)) @function_doc_str))        \"\"\"        doc_str_query = self.language.query(query_code)        doc_strs = doc_str_query.captures(node)        if doc_strs:            return doc_strs[0][0].text.decode()        else:            return None"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 9, "line_from": 9, "line_to": 12, "context": {"module": "treesitter_py", "file_path": "codeqai/treesitter/treesitter_py.py", "file_name": "treesitter_py.py", "class_name": "TreesitterPython", "function_name": null, "snippet": "def __init__(self):        super().__init__(            Language.PYTHON, \"function_definition\", \"identifier\", \"expression_statement\"        )"}}
{"name": "parse", "signature": "def parse(self, file_bytes):", "code_type": "Function", "docstring": null, "line": 14, "line_from": 14, "line_to": 22, "context": {"module": "treesitter_py", "file_path": "codeqai/treesitter/treesitter_py.py", "file_name": "treesitter_py.py", "class_name": "TreesitterPython", "function_name": null, "snippet": "def parse(self, file_bytes: bytes) -> list[TreesitterMethodNode]:        self.tree = self.parser.parse(file_bytes)        result = []        methods = self._query_all_methods(self.tree.root_node)        for method in methods:            method_name = self._query_method_name(method)            doc_comment = self._query_doc_comment(method)            result.append(TreesitterMethodNode(method_name, doc_comment, None, method))        return result"}}
{"name": "_query_method_name", "signature": "def _query_method_name(self, node):", "code_type": "Function", "docstring": null, "line": 24, "line_from": 24, "line_to": 29, "context": {"module": "treesitter_py", "file_path": "codeqai/treesitter/treesitter_py.py", "file_name": "treesitter_py.py", "class_name": "TreesitterPython", "function_name": null, "snippet": "def _query_method_name(self, node: tree_sitter.Node):        if node.type == self.method_declaration_identifier:            for child in node.children:                if child.type == self.method_name_identifier:                    return child.text.decode()        return None"}}
{"name": "_query_all_methods", "signature": "def _query_all_methods(self, node):", "code_type": "Function", "docstring": null, "line": 31, "line_from": 31, "line_to": 41, "context": {"module": "treesitter_py", "file_path": "codeqai/treesitter/treesitter_py.py", "file_name": "treesitter_py.py", "class_name": "TreesitterPython", "function_name": null, "snippet": "def _query_all_methods(self, node: tree_sitter.Node):        methods = []        for child in node.children:            if child.type == self.method_declaration_identifier:                methods.append(child)            if child.type == \"class_definition\":                class_body = child.children[-1]                for child_node in class_body.children:                    if child_node.type == self.method_declaration_identifier:                        methods.append(child_node)        return methods"}}
{"name": "_query_doc_comment", "signature": "def _query_doc_comment(self, node):", "code_type": "Function", "docstring": null, "line": 43, "line_from": 43, "line_to": 54, "context": {"module": "treesitter_py", "file_path": "codeqai/treesitter/treesitter_py.py", "file_name": "treesitter_py.py", "class_name": "TreesitterPython", "function_name": null, "snippet": "def _query_doc_comment(self, node: tree_sitter.Node):        query_code = \"\"\"            (function_definition                body: (block . (expression_statement (string)) @function_doc_str))        \"\"\"        doc_str_query = self.language.query(query_code)        doc_strs = doc_str_query.captures(node)        if doc_strs:            return doc_strs[0][0].text.decode()        else:            return None"}}
{"name": "TreesitterRuby", "signature": "class TreesitterRuby(Treesitter):", "code_type": "Class", "docstring": null, "line": 8, "line_from": 8, "line_to": 35, "context": {"module": "treesitter_rb", "file_path": "codeqai/treesitter/treesitter_rb.py", "file_name": "treesitter_rb.py", "class_name": "TreesitterRuby", "function_name": null, "snippet": "class TreesitterRuby(Treesitter):    def __init__(self):        super().__init__(            Language.RUBY, \"method\", \"identifier\", \"comment\"        )    def parse(self, file_bytes: bytes) -> list[TreesitterMethodNode]:        return super().parse(file_bytes)    def _query_all_methods(        self,        node: tree_sitter.Node,    ):        methods = []        if node.type == self.method_declaration_identifier:            doc_comment = []            doc_comment_node = node            while (                doc_comment_node.prev_named_sibling                and doc_comment_node.prev_named_sibling.type == self.doc_comment_identifier            ):                doc_comment_node = doc_comment_node.prev_named_sibling                doc_comment.insert(0, doc_comment_node.text.decode())            methods.append({\"method\": node, \"doc_comment\": \"\\n\".join(doc_comment)})        else:            for child in node.children:                methods.extend(self._query_all_methods(child))        return methods"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 9, "line_from": 9, "line_to": 12, "context": {"module": "treesitter_rb", "file_path": "codeqai/treesitter/treesitter_rb.py", "file_name": "treesitter_rb.py", "class_name": "TreesitterRuby", "function_name": null, "snippet": "def __init__(self):        super().__init__(            Language.RUBY, \"method\", \"identifier\", \"comment\"        )"}}
{"name": "parse", "signature": "def parse(self, file_bytes):", "code_type": "Function", "docstring": null, "line": 14, "line_from": 14, "line_to": 15, "context": {"module": "treesitter_rb", "file_path": "codeqai/treesitter/treesitter_rb.py", "file_name": "treesitter_rb.py", "class_name": "TreesitterRuby", "function_name": null, "snippet": "def parse(self, file_bytes: bytes) -> list[TreesitterMethodNode]:        return super().parse(file_bytes)"}}
{"name": "_query_all_methods", "signature": "def _query_all_methods(self, node):", "code_type": "Function", "docstring": null, "line": 17, "line_from": 17, "line_to": 35, "context": {"module": "treesitter_rb", "file_path": "codeqai/treesitter/treesitter_rb.py", "file_name": "treesitter_rb.py", "class_name": "TreesitterRuby", "function_name": null, "snippet": "def _query_all_methods(        self,        node: tree_sitter.Node,    ):        methods = []        if node.type == self.method_declaration_identifier:            doc_comment = []            doc_comment_node = node            while (                doc_comment_node.prev_named_sibling                and doc_comment_node.prev_named_sibling.type == self.doc_comment_identifier            ):                doc_comment_node = doc_comment_node.prev_named_sibling                doc_comment.insert(0, doc_comment_node.text.decode())            methods.append({\"method\": node, \"doc_comment\": \"\\n\".join(doc_comment)})        else:            for child in node.children:                methods.extend(self._query_all_methods(child))        return methods"}}
{"name": "TreesitterRegistry", "signature": "class TreesitterRegistry():", "code_type": "Class", "docstring": null, "line": 4, "line_from": 4, "line_to": 17, "context": {"module": "treesitter_registry", "file_path": "codeqai/treesitter/treesitter_registry.py", "file_name": "treesitter_registry.py", "class_name": "TreesitterRegistry", "function_name": null, "snippet": "class TreesitterRegistry:    _registry = {}    @classmethod    def register_treesitter(cls, name, treesitter_class):        cls._registry[name] = treesitter_class    @classmethod    def create_treesitter(cls, name: Language):        treesitter_class = cls._registry.get(name)        if treesitter_class:            return treesitter_class()        else:            raise ValueError(\"Invalid tree type\")"}}
{"name": "register_treesitter", "signature": "def register_treesitter(cls, name, treesitter_class):", "code_type": "Function", "docstring": null, "line": 8, "line_from": 8, "line_to": 9, "context": {"module": "treesitter_registry", "file_path": "codeqai/treesitter/treesitter_registry.py", "file_name": "treesitter_registry.py", "class_name": "TreesitterRegistry", "function_name": null, "snippet": "def register_treesitter(cls, name, treesitter_class):        cls._registry[name] = treesitter_class"}}
{"name": "create_treesitter", "signature": "def create_treesitter(cls, name):", "code_type": "Function", "docstring": null, "line": 12, "line_from": 12, "line_to": 17, "context": {"module": "treesitter_registry", "file_path": "codeqai/treesitter/treesitter_registry.py", "file_name": "treesitter_registry.py", "class_name": "TreesitterRegistry", "function_name": null, "snippet": "def create_treesitter(cls, name: Language):        treesitter_class = cls._registry.get(name)        if treesitter_class:            return treesitter_class()        else:            raise ValueError(\"Invalid tree type\")"}}
{"name": "TreesitterRust", "signature": "class TreesitterRust(Treesitter):", "code_type": "Class", "docstring": null, "line": 8, "line_from": 8, "line_to": 44, "context": {"module": "treesitter_rs", "file_path": "codeqai/treesitter/treesitter_rs.py", "file_name": "treesitter_rs.py", "class_name": "TreesitterRust", "function_name": null, "snippet": "class TreesitterRust(Treesitter):    def __init__(self):        super().__init__(Language.RUST, \"function_item\", \"identifier\", \"line_comment\")    def _query_all_methods(self, node: tree_sitter.Node):        methods = []        if node.type == self.method_declaration_identifier:            doc_comment_nodes = []            if (                node.prev_named_sibling                and node.prev_named_sibling.type == self.doc_comment_identifier            ):                current_doc_comment_node = node.prev_named_sibling                while (                    current_doc_comment_node                    and current_doc_comment_node.type == self.doc_comment_identifier                ):                    doc_comment_nodes.append(current_doc_comment_node.text.decode())                    if current_doc_comment_node.prev_named_sibling:                        current_doc_comment_node = (                            current_doc_comment_node.prev_named_sibling                        )                    else:                        current_doc_comment_node = None            doc_comment_str = \"\"            doc_comment_nodes.reverse()            for doc_comment_node in doc_comment_nodes:                doc_comment_str += doc_comment_node + \"\\n\"            if doc_comment_str.strip() != \"\":                methods.append({\"method\": node, \"doc_comment\": doc_comment_str.strip()})            else:                methods.append({\"method\": node, \"doc_comment\": None})        else:            for child in node.children:                methods.extend(self._query_all_methods(child))        return methods"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 9, "line_from": 9, "line_to": 10, "context": {"module": "treesitter_rs", "file_path": "codeqai/treesitter/treesitter_rs.py", "file_name": "treesitter_rs.py", "class_name": "TreesitterRust", "function_name": null, "snippet": "def __init__(self):        super().__init__(Language.RUST, \"function_item\", \"identifier\", \"line_comment\")"}}
{"name": "_query_all_methods", "signature": "def _query_all_methods(self, node):", "code_type": "Function", "docstring": null, "line": 12, "line_from": 12, "line_to": 44, "context": {"module": "treesitter_rs", "file_path": "codeqai/treesitter/treesitter_rs.py", "file_name": "treesitter_rs.py", "class_name": "TreesitterRust", "function_name": null, "snippet": "def _query_all_methods(self, node: tree_sitter.Node):        methods = []        if node.type == self.method_declaration_identifier:            doc_comment_nodes = []            if (                node.prev_named_sibling                and node.prev_named_sibling.type == self.doc_comment_identifier            ):                current_doc_comment_node = node.prev_named_sibling                while (                    current_doc_comment_node                    and current_doc_comment_node.type == self.doc_comment_identifier                ):                    doc_comment_nodes.append(current_doc_comment_node.text.decode())                    if current_doc_comment_node.prev_named_sibling:                        current_doc_comment_node = (                            current_doc_comment_node.prev_named_sibling                        )                    else:                        current_doc_comment_node = None            doc_comment_str = \"\"            doc_comment_nodes.reverse()            for doc_comment_node in doc_comment_nodes:                doc_comment_str += doc_comment_node + \"\\n\"            if doc_comment_str.strip() != \"\":                methods.append({\"method\": node, \"doc_comment\": doc_comment_str.strip()})            else:                methods.append({\"method\": node, \"doc_comment\": None})        else:            for child in node.children:                methods.extend(self._query_all_methods(child))        return methods"}}
{"name": "TreesitterTypescript", "signature": "class TreesitterTypescript(Treesitter):", "code_type": "Class", "docstring": null, "line": 6, "line_from": 6, "line_to": 10, "context": {"module": "treesitter_ts", "file_path": "codeqai/treesitter/treesitter_ts.py", "file_name": "treesitter_ts.py", "class_name": "TreesitterTypescript", "function_name": null, "snippet": "class TreesitterTypescript(Treesitter):    def __init__(self):        super().__init__(            Language.TYPESCRIPT, \"function_declaration\", \"identifier\", \"comment\"        )"}}
{"name": "__init__", "signature": "def __init__(self):", "code_type": "Function", "docstring": null, "line": 7, "line_from": 7, "line_to": 10, "context": {"module": "treesitter_ts", "file_path": "codeqai/treesitter/treesitter_ts.py", "file_name": "treesitter_ts.py", "class_name": "TreesitterTypescript", "function_name": null, "snippet": "def __init__(self):        super().__init__(            Language.TYPESCRIPT, \"function_declaration\", \"identifier\", \"comment\"        )"}}
{"name": "pytest_configure", "signature": "def pytest_configure(config):", "code_type": "Function", "docstring": null, "line": 8, "line_from": 8, "line_to": 21, "context": {"module": "conftest", "file_path": "tests/conftest.py", "file_name": "conftest.py", "class_name": null, "function_name": null, "snippet": "def pytest_configure(config):    try:        subprocess.run(            [                sys.executable,                \"-m\",                \"pip\",                \"install\",                \"faiss-cpu\",            ],            check=True,        )    except subprocess.CalledProcessError as e:        print(f\"Error during faiss installation: {e}\")"}}
{"name": "test_index_documents", "signature": "def test_index_documents(vector_entries):", "code_type": "Function", "docstring": null, "line": 12, "line_from": 12, "line_to": 34, "context": {"module": "vector_store_test", "file_path": "tests/vector_store_test.py", "file_name": "vector_store_test.py", "class_name": null, "function_name": null, "snippet": "def test_index_documents(vector_entries):    Path(get_cache_path()).mkdir(parents=True, exist_ok=True)    embeddings = FakeEmbeddings(size=1024)    vector_store = VectorStore(name=\"test\", embeddings=embeddings)    vector_store.index_documents(vector_entries)    assert len(vector_store.vector_cache) == 3    assert len(vector_store.db.index_to_docstore_id) == 4    assert (        vector_store.db.docstore.search(vector_store.db.index_to_docstore_id[0])        == vector_entries[0]    )    assert (        vector_store.db.docstore.search(vector_store.db.index_to_docstore_id[1])        == vector_entries[1]    )    assert (        vector_store.db.docstore.search(vector_store.db.index_to_docstore_id[2])        == vector_entries[2]    )    assert (        vector_store.db.docstore.search(vector_store.db.index_to_docstore_id[3])        == vector_entries[3]    )"}}
{"name": "mock_get_commit_hash", "signature": "def mock_get_commit_hash(file):", "code_type": "Function", "docstring": null, "line": 37, "line_from": 37, "line_to": 47, "context": {"module": "vector_store_test", "file_path": "tests/vector_store_test.py", "file_name": "vector_store_test.py", "class_name": null, "function_name": null, "snippet": "def mock_get_commit_hash(file):    if file == \"test.py\":        return \"1234567892\"    elif file == \"fixed_test.py\":        return \"1234567891\"    elif file == \"modified_test.py\":        return \"1234567893\"    elif file == \"new_test.py\":        return \"1234567894\"    else:        return \"1234567890\""}}
{"name": "parse_code_files", "signature": "def parse_code_files(files):", "code_type": "Function", "docstring": null, "line": 50, "line_from": 50, "line_to": 97, "context": {"module": "vector_store_test", "file_path": "tests/vector_store_test.py", "file_name": "vector_store_test.py", "class_name": null, "function_name": null, "snippet": "def parse_code_files(files):    if files == [\"test.py\"]:        return [            Document(                page_content=\"This is a test document.\",                metadata={                    \"filename\": \"test.py\",                    \"commit_hash\": \"1234567892\",                },            ),            Document(                page_content=\"Some further text.\",                metadata={                    \"filename\": \"test.py\",                    \"commit_hash\": \"1234567892\",                },            ),        ]    elif files == [\"new_test.py\"]:        return [            Document(                page_content=\"This is a new test document.\",                metadata={                    \"filename\": \"new_test.py\",                    \"commit_hash\": \"1234567894\",                },            )        ]    elif files == [\"fixed_test.py\"]:        return [            Document(                page_content=\"This is another test document.\",                metadata={                    \"filename\": \"fixed_test.py\",                    \"commit_hash\": \"1234567891\",                },            )        ]    else:        return [            Document(                page_content=\"This is another test document.\",                metadata={                    \"filename\": \"modified_test.py\",                    \"commit_hash\": \"1234567893\",                },            )        ]"}}
{"name": "test_sync_documents", "signature": "def test_sync_documents(file_names, vector_entries, vector_cache, mocker):", "code_type": "Function", "docstring": null, "line": 101, "line_from": 101, "line_to": 127, "context": {"module": "vector_store_test", "file_path": "tests/vector_store_test.py", "file_name": "vector_store_test.py", "class_name": null, "function_name": null, "snippet": "def test_sync_documents(file_names, vector_entries, vector_cache, mocker):    mocker.patch(        \"codeqai.vector_store.get_commit_hash\", side_effect=mock_get_commit_hash    )    mocker.patch(\"codeqai.vector_store.parse_code_files\", side_effect=parse_code_files)    Path(get_cache_path()).mkdir(parents=True, exist_ok=True)    embeddings = FakeEmbeddings(size=1024)    vector_store = VectorStore(name=\"test\", embeddings=embeddings)    vector_store.index_documents(vector_entries)    assert len(vector_store.db.index_to_docstore_id) == 4    vector_store.vector_cache = vector_cache    for vector_id in vector_store.db.index_to_docstore_id.values():        vector_store.vector_cache[            vector_store.db.docstore.search(vector_id).metadata[\"filename\"]        ].vector_ids.append(vector_id)    vector_store.sync_documents(file_names)    assert len(vector_store.db.index_to_docstore_id) == 5    for vector_id in vector_store.db.index_to_docstore_id.values():        filename = vector_store.db.docstore.search(vector_id).metadata[\"filename\"]        commit_hash = vector_store.db.docstore.search(vector_id).metadata[\"commit_hash\"]        cache_vector_ids = set(vector_store.vector_cache[filename].vector_ids)        cache_commit_hash = vector_store.vector_cache[filename].commit_hash        assert vector_id in cache_vector_ids        assert commit_hash == cache_commit_hash"}}
{"name": "file_names", "signature": "def file_names():", "code_type": "Function", "docstring": null, "line": 8, "line_from": 8, "line_to": 14, "context": {"module": "vector_entries", "file_path": "tests/fixtures/vector_entries.py", "file_name": "vector_entries.py", "class_name": null, "function_name": null, "snippet": "def file_names():    return [        \"fixed_test.py\",        \"test.py\",        \"modified_test.py\",        \"new_test.py\",    ]"}}
{"name": "vector_entries", "signature": "def vector_entries():", "code_type": "Function", "docstring": null, "line": 18, "line_from": 18, "line_to": 48, "context": {"module": "vector_entries", "file_path": "tests/fixtures/vector_entries.py", "file_name": "vector_entries.py", "class_name": null, "function_name": null, "snippet": "def vector_entries():    return [        Document(            page_content=\"This is a test document.\",            metadata={                \"filename\": \"test.py\",                \"commit_hash\": \"1234567890\",            },        ),        Document(            page_content=\"Some further text.\",            metadata={                \"filename\": \"test.py\",                \"commit_hash\": \"1234567890\",            },        ),        Document(            page_content=\"This is another test document.\",            metadata={                \"filename\": \"another_test.py\",                \"commit_hash\": \"1234567891\",            },        ),        Document(            page_content=\"This is another test document.\",            metadata={                \"filename\": \"fixed_test.py\",                \"commit_hash\": \"1234567891\",            },        ),    ]"}}
{"name": "modified_vector_entries", "signature": "def modified_vector_entries():", "code_type": "Function", "docstring": null, "line": 52, "line_from": 52, "line_to": 89, "context": {"module": "vector_entries", "file_path": "tests/fixtures/vector_entries.py", "file_name": "vector_entries.py", "class_name": null, "function_name": null, "snippet": "def modified_vector_entries():    return [        Document(            page_content=\"This is another test document.\",            metadata={                \"filename\": \"fixed_test.py\",                \"commit_hash\": \"1234567891\",            },        ),        Document(            page_content=\"This is a modified test document.\",            metadata={                \"filename\": \"test.py\",                \"commit_hash\": \"1234567892\",            },        ),        Document(            page_content=\"Some further modified text.\",            metadata={                \"filename\": \"test.py\",                \"commit_hash\": \"1234567892\",            },        ),        Document(            page_content=\"This is another modified test document.\",            metadata={                \"filename\": \"modified_test.py\",                \"commit_hash\": \"1234567893\",            },        ),        Document(            page_content=\"This is a new test document.\",            metadata={                \"filename\": \"new_test.py\",                \"commit_hash\": \"1234567894\",            },        ),    ]"}}
{"name": "vector_cache", "signature": "def vector_cache():", "code_type": "Function", "docstring": null, "line": 93, "line_from": 93, "line_to": 110, "context": {"module": "vector_entries", "file_path": "tests/fixtures/vector_entries.py", "file_name": "vector_entries.py", "class_name": null, "function_name": null, "snippet": "def vector_cache():    return {        \"test.py\": VectorCache(            filename=\"test.py\",            vector_ids=[],            commit_hash=\"1234567890\",        ),        \"another_test.py\": VectorCache(            filename=\"another_test.py\",            vector_ids=[],            commit_hash=\"1234567891\",        ),        \"fixed_test.py\": VectorCache(            filename=\"fixed_test.py\",            vector_ids=[],            commit_hash=\"1234567891\",        ),    }"}}
