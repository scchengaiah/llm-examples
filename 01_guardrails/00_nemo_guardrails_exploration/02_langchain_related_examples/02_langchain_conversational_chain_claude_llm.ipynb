{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "set_debug(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_debug\n",
    "set_debug(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Conversational Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "CONDENSE_QUESTION_PROMPT_STR = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in English language. Avoid presenting empty standalone questions. If ambiguity arises, retain the follow up question as is. Do not include any other content other than the rephrased question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = ChatPromptTemplate.from_template(CONDENSE_QUESTION_PROMPT_STR)\n",
    "\n",
    "QA_PROMPT_STR = \"\"\"You are a friendly chatbot assistant that responds in a conversational manner to users' question on company's policies. \n",
    "Respond in 1-2 complete sentences, unless specifically asked by the user to elaborate on something. Use \"Context\" to inform your answers.\n",
    "Do not make up answers if the question is out of \"Context\". Do not respond with any general information or advice that is not related to the context.\n",
    "Respond to greetings or compliments in a positive manner and let the user know your capability.\n",
    "\n",
    "---\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Question:\n",
    "{question}\n",
    "---\n",
    "Response:\n",
    "\"\"\"\n",
    "QA_PROMPT = ChatPromptTemplate.from_template(QA_PROMPT_STR)\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Azure AI search vector store and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from langchain_community.embeddings.bedrock import BedrockEmbeddings\n",
    "\n",
    "azure_search_endpoint = os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "azure_search_api_key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "azure_search_index = os.getenv(\"AZURE_SEARCH_INDEX\")\n",
    "\n",
    "azure_embedding_deployment = os.getenv(\"AZURE_EMBEDDING_MODEL_DEPLOYMENT_NAME\")\n",
    "\n",
    "embeddings = BedrockEmbeddings(region_name = os.getenv(\"AWS_REGION\"), model_id= os.getenv(\"AWS_LLM_EMBEDDINGS_ID\"))\n",
    "\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=azure_search_endpoint,\n",
    "    azure_search_key=azure_search_api_key,\n",
    "    index_name=azure_search_index,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize LLM object - Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models.bedrock import BedrockChat\n",
    "import os\n",
    "\n",
    "llm = BedrockChat(\n",
    "            region_name=os.getenv(\"AWS_REGION\"),\n",
    "            model_id=os.getenv(\"AWS_CLAUDE_LLM_MODEL_ID\"),\n",
    "            streaming=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize document handling after retrieval and preparation of context for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.prompts import format_document\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "def combine_documents(docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"):\n",
    "        doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "        return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "# Takes the standalone question as the input and the context as the vectorstore.\n",
    "# Confine our retrieval to Germany policies loaded.\n",
    "search_kwargs={\"filters\": \"location eq 'Germany'\",\"k\":3}\n",
    "context = {\n",
    "    \"context\": itemgetter(\"question\") | vector_store.as_retriever(search_kwargs= search_kwargs) | combine_documents,\n",
    "    \"question\": lambda x: x[\"question\"],\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain considering `chat history` and generation of the follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Output of the _inputs execution is the standalone question in the format:\n",
    "# {\"question\": \"question\"}\n",
    "inputs = RunnableParallel(\n",
    "    question=RunnablePassthrough.assign(\n",
    "        chat_history=lambda x: get_buffer_string(x[\"chat_history\"])\n",
    "    )\n",
    "                        | CONDENSE_QUESTION_PROMPT\n",
    "                        | llm\n",
    "                        | StrOutputParser(),\n",
    ")\n",
    "\n",
    "# chain with follow-up question considered. This indicates that the length of the chat_history list is greater than 0.\n",
    "chain_with_follow_up_question = inputs | context | QA_PROMPT | llm\n",
    "chain_with_follow_up_question_without_llm = inputs | context | QA_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain without considering `chat history` and generation of the follow-up question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_without_follow_up_question = context | QA_PROMPT | llm\n",
    "chain_without_follow_up_question_without_llm = context | QA_PROMPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with various inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without follow-up question chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat_history not provided.\n",
    "#result = chain_without_follow_up_question.invoke({\"question\": \"What is the capital of France?\"})\n",
    "result = await chain_without_follow_up_question.ainvoke({\"question\": \"What is the capital of France?\"})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain_without_follow_up_question.stream({\"question\": \"Explain our company's leave policy ?\"}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With follow-up question chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "\n",
    "query = \"Explain travel allowance policy\"\n",
    "chat_history =  [\n",
    "                    HumanMessage(content=\"Explain our company's leave policy ?\"), \n",
    "                    AIMessage(content=\"Employees are eligible for 30 days of regular leaves for a given calendar year (1st Jan till 31st Dec) and must apply for planned leaves with prior approval from their project manager and designated reporting manager. In case of emergency, employees must inform their immediate superior and HR, and all leaves must be applied through the Intelizign Intranet Portal.\"),\n",
    "                    HumanMessage(content=\"Explain our company loan policy\"), \n",
    "                    AIMessage(content=\"Our loan policy allows relocated employees in Germany to request a loan for a flat deposit up to 3,000â‚¬. To request a loan, employees must email the HR department with the purpose and required amount, and sign a document prepared by HR before repayment within one financial year.\")\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = chain_with_follow_up_question.invoke({\"question\": query, \"chat_history\": chat_history})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chain_with_follow_up_question.stream({\"question\": query, \"chat_history\": chat_history}):\n",
    "    print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeMo Guardrails setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch so that async/await calls work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardrails without streaming example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.load.load import loads\n",
    "\n",
    "# https://github.com/NVIDIA/NeMo-Guardrails/blob/88da745847355c97be5f3279e9d04275754e6c48/docs/user_guides/langchain/runnable-as-action/README.md\n",
    "class ExecuteQAChainRunnable(Runnable):\n",
    "    def invoke(self, input, config = None, **kwargs):\n",
    "        chat_history = loads(input[\"chat_history\"])\n",
    "        chain_input = {\"question\": input[\"question\"], \"chat_history\": chat_history}\n",
    "\n",
    "        if len(chat_history) > 0:\n",
    "            result = chain_with_follow_up_question.invoke(chain_input)\n",
    "        else:\n",
    "            result = chain_without_follow_up_question.invoke(chain_input)\n",
    "\n",
    "        return result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.load.dump import dumps\n",
    "\n",
    "config = RailsConfig.from_path(\"config_openai\")\n",
    "\n",
    "# We go with Azure OpenAI LLM considering the optimization of prompts with Bedrock.\n",
    "rails = LLMRails(config, llm=llm)\n",
    "\n",
    "rails.register_action(ExecuteQAChainRunnable(), \"qa_chain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "query = \"I want to go for a vacation. how many leaves are allowed ?\"\n",
    "query = \"Explain travel allowance policy\"\n",
    "chat_history =  [\n",
    "                    HumanMessage(content=\"Explain our company's leave policy ?\"), \n",
    "                    AIMessage(content=\"Employees are eligible for 30 days of regular leaves for a given calendar year (1st Jan till 31st Dec) and must apply for planned leaves with prior approval from their project manager and designated reporting manager. In case of emergency, employees must inform their immediate superior and HR, and all leaves must be applied through the Intelizign Intranet Portal.\"),\n",
    "                    HumanMessage(content=\"Explain our company loan policy\"), \n",
    "                    AIMessage(content=\"Our loan policy allows relocated employees in Germany to request a loan for a flat deposit up to 3,000â‚¬. To request a loan, employees must email the HR department with the purpose and required amount, and sign a document prepared by HR before repayment within one financial year.\")\n",
    "                ]\n",
    "                \n",
    "#chat_history = []\n",
    "\n",
    "messages = [{\"role\": \"context\", \"content\": {\"chat_history\": dumps(chat_history)}},\n",
    "            {\"role\": \"user\",\"content\": query}]\n",
    "\n",
    "response = rails.generate(messages=messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardrails with streaming example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "import os\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "            azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "            deployment_name=os.getenv(\"AZURE_LLM_MODEL_DEPLOYMENT_NAME\"),\n",
    "            temperature=0,\n",
    "            max_tokens=1000,\n",
    "            streaming=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.bedrock import Bedrock\n",
    "from langchain_community.chat_models.bedrock import BedrockChat\n",
    "import os\n",
    "\n",
    "# Working version\n",
    "llm = Bedrock(\n",
    "            region_name=os.getenv(\"AWS_REGION\"),\n",
    "            #model_id=os.getenv(\"AWS_CLAUDE_LLM_MODEL_ID\"),\n",
    "            model_id=\"anthropic.claude-instant-v1\",\n",
    "            streaming=True\n",
    "        )\n",
    "\n",
    "# WITH BEDROCKCHAT; STREAMING CAPABILITIES OF GUARDRAILS ARE NOT WORKING AS EXPECTED.\n",
    "# THIS RESULTS IN THE USER INTENT BEING INCORRECTLY PRODUCED, IF GOING WITH BEDROCK, IT WOULD BE BETTER TO USE\n",
    "# THE ABOVE VERSION OF THE CODE USING V1 OF THE CLAUDE MODEL.\n",
    "\n",
    "#llm = BedrockChat(\n",
    "#            region_name=os.getenv(\"AWS_REGION\"),\n",
    "#            #model_id=os.getenv(\"AWS_CLAUDE_LLM_MODEL_ID\"),\n",
    "#            model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "#            streaming=True\n",
    "#        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from langchain_core.load.load import loads\n",
    "from nemoguardrails.actions import action\n",
    "from langchain_core.language_models import BaseLLM\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from nemoguardrails.streaming import StreamingHandler\n",
    "from nemoguardrails.context import streaming_handler_var\n",
    "\n",
    "@action(is_system_action=True)\n",
    "async def call_llm_chain(user_query: str, chat_history) -> str:\n",
    "    call_config = RunnableConfig(callbacks=[streaming_handler_var.get()])\n",
    "    chat_history_parsed = loads(chat_history)\n",
    "    chain_input = {\"question\": user_query, \"chat_history\": chat_history_parsed}\n",
    "\n",
    "    if streaming_handler_var.get() is not None:\n",
    "        if llm.callbacks is None: \n",
    "            llm.callbacks = [streaming_handler_var.get()] \n",
    "        else:\n",
    "            llm.callbacks.extend([streaming_handler_var.get()])\n",
    "\n",
    "    \"\"\"\n",
    "    response = await llm.ainvoke(user_query, config=call_config)\n",
    "    return response\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(chat_history_parsed) > 0:\n",
    "        updated_chain = chain_with_follow_up_question_without_llm | llm\n",
    "        result = await updated_chain.ainvoke(chain_input)\n",
    "    else:\n",
    "        updated_chain = chain_without_follow_up_question_without_llm | llm\n",
    "        result = await updated_chain.ainvoke(chain_input)\n",
    "\n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.load.dump import dumps\n",
    "\n",
    "config = RailsConfig.from_path(\"config_claude\")\n",
    "\n",
    "# We go with Azure OpenAI LLM considering the optimization of prompts with Bedrock.\n",
    "rails = LLMRails(config, llm=llm)\n",
    "\n",
    "# Register custom action\n",
    "rails.register_action(call_llm_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from langchain_core.messages.human import HumanMessage\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "from langchain_core.load.dump import dumps\n",
    "import asyncio\n",
    "import uuid\n",
    "from nemoguardrails.context import streaming_handler_var\n",
    "import os\n",
    "\n",
    "# Handle tokens returned from the streaming handler and print the chunks.\n",
    "async def process_tokens(streaming_handler):\n",
    "    async for chunk in streaming_handler:\n",
    "        print(chunk, end=\"\", flush=True)\n",
    "        # Or do something else with the token\n",
    "\n",
    "# Handle all the execution logic within the async function.\n",
    "async def demo_streaming_from_custom_action(query):\n",
    "    \n",
    "    streaming_handler = StreamingHandler()\n",
    "    streaming_handler_var.set(streaming_handler)\n",
    "    streaming_task = asyncio.create_task(process_tokens(streaming_handler))\n",
    "\n",
    "    print(\"*\" * 25)\n",
    "    print(\"Executing Query:\")\n",
    "    print(query)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    chat_history = [\n",
    "        HumanMessage(content=\"Explain our company's leave policy ?\"),\n",
    "        AIMessage(\n",
    "            content=\"Employees are eligible for 30 days of regular leaves for a given calendar year (1st Jan till 31st Dec) and must apply for planned leaves with prior approval from their project manager and designated reporting manager. In case of emergency, employees must inform their immediate superior and HR, and all leaves must be applied through the Intelizign Intranet Portal.\"),\n",
    "        HumanMessage(content=\"Explain our company loan policy\"),\n",
    "        AIMessage(\n",
    "            content=\"Our loan policy allows relocated employees in Germany to request a loan for a flat deposit up to 3,000â‚¬. To request a loan, employees must email the HR department with the purpose and required amount, and sign a document prepared by HR before repayment within one financial year.\")\n",
    "    ]\n",
    "\n",
    "    #chat_history = []\n",
    "\n",
    "    messages = [{\"role\": \"context\", \"content\": {\"chat_history\": dumps(chat_history)}},\n",
    "                {\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "\n",
    "    result = await rails.generate_async(\n",
    "        messages=messages, streaming_handler=streaming_handler\n",
    "    )\n",
    "    await asyncio.gather(streaming_task)\n",
    "    #print(\"\\n\")\n",
    "    #print(\"RESULT:\")\n",
    "    #print(result)\n",
    "    #print(\"\\n\")\n",
    "    #return result\n",
    "\n",
    "#query = \"I want to go for a vacation. how many leaves are allowed ?\"\n",
    "#query = \"One of my close family member has passed away. which type of leave can I apply for the same ?\"\n",
    "#query = \"Explain about Albert Einstein in 200 words\"\n",
    "query = \"Explain our language policy for German in detail. Also share the reimbursement amount offered\"\n",
    "\n",
    "asyncio.run(demo_streaming_from_custom_action(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the rails execution related aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = rails.explain()\n",
    "print(info.colang_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = rails.explain()\n",
    "info.print_llm_calls_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = rails.explain()\n",
    "for llm_call in info.llm_calls:\n",
    "    print(\"*\" * 25 + \"PROMPT\" + \"*\" * 25)\n",
    "    print(llm_call.prompt)\n",
    "\n",
    "    print(\"*\" * 25 + \"COMPLETION\" + \"*\" * 25)\n",
    "    print(llm_call.completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "\n",
    "[Using NVIDIA NeMo Guardrails with Amazon Bedrock](https://www.linkedin.com/pulse/using-nvidia-nemo-guardrails-amazon-bedrock-khobaib-zaamout-ph-d--b57hc?utm_source=share&utm_medium=member_android&utm_campaign=share_via)\n",
    "\n",
    "[Using NVIDIA NeMo Guardrails with Amazon Bedrock - AWS Reference](https://community.aws/content/2e8kWQ7TihDbxj8ei22DKi2pfFf/using-nvidia-nemo-guardrails-with-bedrock)\n",
    "\n",
    "[Amazon Bedrock support - Github Issue](https://github.com/NVIDIA/NeMo-Guardrails/issues/118)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
