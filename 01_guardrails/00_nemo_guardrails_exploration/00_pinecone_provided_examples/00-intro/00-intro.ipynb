{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We installed the below dependencies via requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "OWG3JPB0jUJo"
      },
      "outputs": [],
      "source": [
        "#!pip install -qU \\\n",
        "#    nemoguardrails==0.4.0 \\\n",
        "#    openai==0.27.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDrsyThjjUJp"
      },
      "source": [
        "We need to set our OpenAI API key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oHQOTiTQjUJp"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(dotenv_path=\"../../.env\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jcm8FnQ_jUJp"
      },
      "source": [
        "We can run Guardrails from terminal using the command:\n",
        "\n",
        "```\n",
        "nemoguardrails chat --config=config/\n",
        "```\n",
        "\n",
        "Where the `config` directory must contain our `config.yml` and a Colang file (like `topics.co`).\n",
        "\n",
        "Alternatively we can load them from file using `RailsConfig.from_path(\"./config\")` or from string variables in our code like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DhZ9qy90jUJq"
      },
      "outputs": [],
      "source": [
        "yaml_content = \"\"\"\n",
        "models:\n",
        "- type: main\n",
        "  engine: azure\n",
        "  model: gpt-3.5\n",
        "\"\"\"\n",
        "colang_content = \"\"\"\n",
        "# define niceties\n",
        "define user express greeting\n",
        "    \"hello\"\n",
        "    \"hi\"\n",
        "    \"what's up?\"\n",
        "\n",
        "# define limits\n",
        "define user ask politics\n",
        "    \"what are your political beliefs?\"\n",
        "    \"thoughts on the president?\"\n",
        "    \"left wing\"\n",
        "    \"right wing\"\n",
        "\n",
        "define bot answer politics\n",
        "    \"I'm a shopping assistant, I don't like to talk of politics.\"\n",
        "\n",
        "define flow politics\n",
        "    user ask politics\n",
        "    bot answer politics\n",
        "    bot offer help\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "PrKzVn4wjUJq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from nemoguardrails import LLMRails, RailsConfig\n",
        "\n",
        "# https://github.com/NVIDIA/NeMo-Guardrails/issues/72#issuecomment-1645603818\n",
        "from langchain_openai.chat_models import AzureChatOpenAI\n",
        "llm = AzureChatOpenAI(deployment_name=os.getenv(\"AZURE_LLM_MODEL_DEPLOYMENT_NAME\"))\n",
        "\n",
        "\n",
        "# initialize rails config\n",
        "config = RailsConfig.from_content(\n",
        "  \t#yaml_content=yaml_content, # We pass the LLM directly to the LLMRails\n",
        "    colang_content=colang_content\n",
        ")\n",
        "# create rails\n",
        "rails = LLMRails(config, llm = llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdI13hg4jUJq"
      },
      "source": [
        "From here, we begin asking questions and interacting with our Guardrails protected LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htzi4K72jUJq",
        "outputId": "8615c158-9d06-4423-b454-d7489ddb9e30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello again! How can I assist you today?\n",
            "By the way, how are you feeling today?\n"
          ]
        }
      ],
      "source": [
        "import logging\n",
        "\n",
        "# logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "res = await rails.generate_async(prompt=\"Hey there!\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2qv5mEujUJq"
      },
      "source": [
        "This is a typical greeting so we have no protective guardrails here. However, we do see the `greeting` flow being activated as the chatbot generates some text from the `bot express greeting` message, and on the next line generates some more text from the `bot ask how are you` message.\n",
        "\n",
        "Let's try asking a more political question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-e24M6PjUJq",
        "outputId": "e8f5c8b4-4a1a-495f-f6a9-4d86864933c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm a shopping assistant, I don't like to talk of politics.\n",
            "If you have any other questions or if there's anything else I can help you with, please let me know.\n"
          ]
        }
      ],
      "source": [
        "res = await rails.generate_async(prompt=\"what do you think of the president?\")\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IizgBc-jUJr"
      },
      "source": [
        "Here we can see that our `politics` rail is activated and our chatbot dodges the question with `bot answer politics` and follows up with `bot offer help`.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
